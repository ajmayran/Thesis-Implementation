{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e1e21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rosem\\Documents\\AJ\\Thesis-Implementation\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully\n",
      "SHAP version: 0.49.1\n",
      "Data Loaded:\n",
      "   Training samples: 888\n",
      "   Test samples: 222\n",
      "   Features: 16\n",
      "   Feature names: 16\n",
      "Loaded: KNN\n",
      "Loaded: DECISION_TREE\n",
      "Loaded: RANDOM_FOREST\n",
      "Loaded: SVR\n",
      "Loaded: RIDGE\n",
      "\n",
      "======================================================================\n",
      "SHAP (SHAPLEY VALUES) FEATURE IMPORTANCE ANALYSIS - BASE MODELS\n",
      "======================================================================\n",
      "\n",
      "Calculating SHAP values for all base models...\n",
      "This may take several minutes...\n",
      "\n",
      "[KNN] Computing SHAP values...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 4/100 [00:02<01:06,  1.44it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     53\u001b[39m     background = shap.sample(X_train, \u001b[32m100\u001b[39m)\n\u001b[32m     54\u001b[39m     explainer = shap.KernelExplainer(model.predict, background)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     shap_values = \u001b[43mexplainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m shap_results[name] = {\n\u001b[32m     58\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mexplainer\u001b[39m\u001b[33m'\u001b[39m: explainer,\n\u001b[32m     59\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mshap_values\u001b[39m\u001b[33m'\u001b[39m: shap_values,\n\u001b[32m     60\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mfeature_importance\u001b[39m\u001b[33m'\u001b[39m: np.abs(shap_values).mean(axis=\u001b[32m0\u001b[39m)\n\u001b[32m     61\u001b[39m }\n\u001b[32m     63\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   SHAP computation complete for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname.upper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rosem\\Documents\\AJ\\Thesis-Implementation\\venv\\Lib\\site-packages\\shap\\explainers\\_kernel.py:275\u001b[39m, in \u001b[36mKernelExplainer.shap_values\u001b[39m\u001b[34m(self, X, **kwargs)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.keep_index:\n\u001b[32m    274\u001b[39m     data = convert_to_instance_with_index(data, column_name, index_value[i : i + \u001b[32m1\u001b[39m], index_name)\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m explanations.append(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexplain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mgc_collect\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    277\u001b[39m     gc.collect()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rosem\\Documents\\AJ\\Thesis-Implementation\\venv\\Lib\\site-packages\\shap\\explainers\\_kernel.py:479\u001b[39m, in \u001b[36mKernelExplainer.explain\u001b[39m\u001b[34m(self, incoming_instance, **kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m     \u001b[38;5;28mself\u001b[39m.kernelWeights[nfixed_samples:] *= weight_left / \u001b[38;5;28mself\u001b[39m.kernelWeights[nfixed_samples:].sum()\n\u001b[32m    478\u001b[39m \u001b[38;5;66;03m# execute the model on the synthetic samples we have created\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# solve then expand the feature importance (Shapley value) vector to contain the non-varying features\u001b[39;00m\n\u001b[32m    482\u001b[39m phi = np.zeros((\u001b[38;5;28mself\u001b[39m.data.groups_size, \u001b[38;5;28mself\u001b[39m.D))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rosem\\Documents\\AJ\\Thesis-Implementation\\venv\\Lib\\site-packages\\shap\\explainers\\_kernel.py:624\u001b[39m, in \u001b[36mKernelExplainer.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    622\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.keep_index_ordered:\n\u001b[32m    623\u001b[39m         data = data.sort_index()\n\u001b[32m--> \u001b[39m\u001b[32m624\u001b[39m modelOut = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(modelOut, (pd.DataFrame, pd.Series)):\n\u001b[32m    626\u001b[39m     modelOut = modelOut.values\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rosem\\Documents\\AJ\\Thesis-Implementation\\venv\\Lib\\site-packages\\sklearn\\neighbors\\_regression.py:246\u001b[39m, in \u001b[36mKNeighborsRegressor.predict\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    244\u001b[39m     neigh_dist = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     neigh_dist, neigh_ind = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    248\u001b[39m weights = _get_weights(neigh_dist, \u001b[38;5;28mself\u001b[39m.weights)\n\u001b[32m    250\u001b[39m _y = \u001b[38;5;28mself\u001b[39m._y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rosem\\Documents\\AJ\\Thesis-Implementation\\venv\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:869\u001b[39m, in \u001b[36mKNeighborsMixin.kneighbors\u001b[39m\u001b[34m(self, X, n_neighbors, return_distance)\u001b[39m\n\u001b[32m    862\u001b[39m use_pairwise_distances_reductions = (\n\u001b[32m    863\u001b[39m     \u001b[38;5;28mself\u001b[39m._fit_method == \u001b[33m\"\u001b[39m\u001b[33mbrute\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    864\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m ArgKmin.is_usable_for(\n\u001b[32m    865\u001b[39m         X \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fit_X, \u001b[38;5;28mself\u001b[39m._fit_X, \u001b[38;5;28mself\u001b[39m.effective_metric_\n\u001b[32m    866\u001b[39m     )\n\u001b[32m    867\u001b[39m )\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_pairwise_distances_reductions:\n\u001b[32m--> \u001b[39m\u001b[32m869\u001b[39m     results = \u001b[43mArgKmin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    870\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    871\u001b[39m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    872\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_neighbors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    873\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meffective_metric_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meffective_metric_params_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m    880\u001b[39m     \u001b[38;5;28mself\u001b[39m._fit_method == \u001b[33m\"\u001b[39m\u001b[33mbrute\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.metric == \u001b[33m\"\u001b[39m\u001b[33mprecomputed\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m issparse(X)\n\u001b[32m    881\u001b[39m ):\n\u001b[32m    882\u001b[39m     results = _kneighbors_from_graph(\n\u001b[32m    883\u001b[39m         X, n_neighbors=n_neighbors, return_distance=return_distance\n\u001b[32m    884\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rosem\\Documents\\AJ\\Thesis-Implementation\\venv\\Lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py:281\u001b[39m, in \u001b[36mArgKmin.compute\u001b[39m\u001b[34m(cls, X, Y, k, metric, chunk_size, metric_kwargs, strategy, return_distance)\u001b[39m\n\u001b[32m    200\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Compute the argkmin reduction.\u001b[39;00m\n\u001b[32m    201\u001b[39m \n\u001b[32m    202\u001b[39m \u001b[33;03mParameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    278\u001b[39m \u001b[33;03mreturns.\u001b[39;00m\n\u001b[32m    279\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X.dtype == Y.dtype == np.float64:\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mArgKmin64\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[43m=\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    284\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmetric_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    288\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    289\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    290\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m X.dtype == Y.dtype == np.float32:\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ArgKmin32.compute(\n\u001b[32m    294\u001b[39m         X=X,\n\u001b[32m    295\u001b[39m         Y=Y,\n\u001b[32m   (...)\u001b[39m\u001b[32m    301\u001b[39m         return_distance=return_distance,\n\u001b[32m    302\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32msklearn/metrics/_pairwise_distances_reduction/_argkmin.pyx:59\u001b[39m, in \u001b[36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\rosem\\Documents\\AJ\\Thesis-Implementation\\venv\\Lib\\site-packages\\threadpoolctl.py:592\u001b[39m, in \u001b[36m_ThreadpoolLimiter.__exit__\u001b[39m\u001b[34m(self, type, value, traceback)\u001b[39m\n\u001b[32m    589\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    590\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m592\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mtype\u001b[39m, value, traceback):\n\u001b[32m    593\u001b[39m     \u001b[38;5;28mself\u001b[39m.restore_original_limits()\n\u001b[32m    595\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m    596\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrap\u001b[39m(\u001b[38;5;28mcls\u001b[39m, controller, *, limits=\u001b[38;5;28;01mNone\u001b[39;00m, user_api=\u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "import shap\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"All libraries imported successfully\")\n",
    "print(\"SHAP version:\", shap.__version__)\n",
    "\n",
    "# Load preprocessed data\n",
    "X_train = np.load('../regression_processed_data/X_train.npy')\n",
    "X_test = np.load('../regression_processed_data/X_test.npy')\n",
    "y_train = np.load('../regression_processed_data/y_train.npy')\n",
    "y_test = np.load('../regression_processed_data/y_test.npy')\n",
    "feature_names = json.load(open('../regression_processed_data/feature_names.json'))\n",
    "\n",
    "print(f\"Data Loaded:\")\n",
    "print(f\"   Training samples: {X_train.shape[0]}\")\n",
    "print(f\"   Test samples: {X_test.shape[0]}\")\n",
    "print(f\"   Features: {X_train.shape[1]}\")\n",
    "print(f\"   Feature names: {len(feature_names)}\")\n",
    "\n",
    "# Dynamically load all available base models\n",
    "models = {}\n",
    "model_dir = '../saved_base_models'\n",
    "\n",
    "# Check if directory exists\n",
    "if not os.path.exists(model_dir):\n",
    "    print(f\"Error: Directory {model_dir} does not exist\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# Get all .pkl files in the directory\n",
    "available_models = [f.replace('_model.pkl', '') for f in os.listdir(model_dir) \n",
    "                   if f.endswith('_model.pkl')]\n",
    "\n",
    "if not available_models:\n",
    "    print(f\"Error: No base models found in {model_dir}\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(f\"\\nFound {len(available_models)} base models:\")\n",
    "for model_name in available_models:\n",
    "    print(f\"   - {model_name}\")\n",
    "\n",
    "# Load all available models\n",
    "for model_name in available_models:\n",
    "    model_path = f'{model_dir}/{model_name}_model.pkl'\n",
    "    try:\n",
    "        models[model_name] = joblib.load(model_path)\n",
    "        print(f\"Loaded: {model_name.upper()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {model_name}: {str(e)}\")\n",
    "\n",
    "if not models:\n",
    "    print(\"Error: No models were successfully loaded\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SHAP (SHAPLEY VALUES) FEATURE IMPORTANCE ANALYSIS - BASE MODELS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "shap_results = {}\n",
    "\n",
    "print(\"\\nCalculating SHAP values for all base models...\")\n",
    "print(\"This may take several minutes...\")\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n[{name.upper()}] Computing SHAP values...\")\n",
    "    \n",
    "    try:\n",
    "        # Determine model type and use appropriate explainer\n",
    "        if 'random_forest' in name.lower() or 'decision_tree' in name.lower():\n",
    "            # Tree-based models use TreeExplainer\n",
    "            explainer = shap.TreeExplainer(model)\n",
    "            shap_values = explainer.shap_values(X_test)\n",
    "        \n",
    "        elif 'neural' in name.lower() or 'mlp' in name.lower():\n",
    "            # Neural network models use KernelExplainer with smaller sample\n",
    "            background = shap.sample(X_train, 100)\n",
    "            explainer = shap.KernelExplainer(model.predict, background)\n",
    "            shap_values = explainer.shap_values(X_test[:100])\n",
    "        \n",
    "        else:\n",
    "            # Other models (KNN, SVR, Ridge, etc.) use KernelExplainer\n",
    "            background = shap.sample(X_train, 100)\n",
    "            explainer = shap.KernelExplainer(model.predict, background)\n",
    "            shap_values = explainer.shap_values(X_test[:100])\n",
    "        \n",
    "        shap_results[name] = {\n",
    "            'explainer': explainer,\n",
    "            'shap_values': shap_values,\n",
    "            'feature_importance': np.abs(shap_values).mean(axis=0)\n",
    "        }\n",
    "        \n",
    "        print(f\"   SHAP computation complete for {name.upper()}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   Error computing SHAP for {name}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "if not shap_results:\n",
    "    print(\"Error: No SHAP values were computed successfully\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SHAP FEATURE IMPORTANCE RANKINGS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "shap_importance_df_list = []\n",
    "\n",
    "for name, shap_data in shap_results.items():\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'SHAP_Importance': shap_data['feature_importance']\n",
    "    }).sort_values('SHAP_Importance', ascending=False)\n",
    "    \n",
    "    shap_importance_df_list.append(importance_df)\n",
    "    \n",
    "    print(f\"\\n{name.upper()} - Top 10 Features by SHAP:\")\n",
    "    print(f\"{'Rank':<6}{'Feature':<25}{'SHAP Importance':<20}\")\n",
    "    print(\"-\" * 51)\n",
    "    \n",
    "    for i, (idx, row) in enumerate(importance_df.head(10).iterrows(), 1):\n",
    "        print(f\"{i:<6}{row['Feature']:<25}{row['SHAP_Importance']:<20.6f}\")\n",
    "\n",
    "# Combine SHAP importance across all models\n",
    "combined_shap = pd.DataFrame({\n",
    "    'Feature': feature_names\n",
    "})\n",
    "\n",
    "for name, shap_data in shap_results.items():\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        f'{name}_shap': shap_data['feature_importance']\n",
    "    })\n",
    "    combined_shap = combined_shap.merge(importance_df, on='Feature')\n",
    "\n",
    "shap_cols = [col for col in combined_shap.columns if col.endswith('_shap')]\n",
    "combined_shap['avg_shap'] = combined_shap[shap_cols].mean(axis=1)\n",
    "combined_shap = combined_shap.sort_values('avg_shap', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CONSENSUS SHAP FEATURE IMPORTANCE (Averaged Across All Base Models)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n{'Rank':<6}{'Feature':<25}{'Average SHAP':<20}\")\n",
    "print(\"-\" * 51)\n",
    "\n",
    "for i, (idx, row) in enumerate(combined_shap.head(12).iterrows(), 1):\n",
    "    print(f\"{i:<6}{row['Feature']:<25}{row['avg_shap']:<20.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SHAP VISUALIZATIONS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs('../saved_base_models/shap_analysis', exist_ok=True)\n",
    "\n",
    "# Calculate grid size for subplots\n",
    "num_models = len(shap_results)\n",
    "num_cols = 3\n",
    "num_rows = (num_models + 1 + num_cols - 1) // num_cols\n",
    "\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 6 * num_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "colors = ['#2ecc71', '#3498db', '#e74c3c', '#9b59b6', '#f39c12', '#1abc9c', '#e67e22', '#34495e']\n",
    "\n",
    "for idx, (name, shap_data) in enumerate(shap_results.items()):\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'SHAP_Importance': shap_data['feature_importance']\n",
    "    }).sort_values('SHAP_Importance', ascending=False).head(10)\n",
    "    \n",
    "    axes[idx].barh(importance_df['Feature'], importance_df['SHAP_Importance'],\n",
    "                   color=colors[idx % len(colors)], edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_xlabel('Mean |SHAP Value|', fontweight='bold')\n",
    "    axes[idx].set_title(f'{name.upper()} - SHAP Feature Importance', fontweight='bold')\n",
    "    axes[idx].invert_yaxis()\n",
    "    axes[idx].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add consensus plot\n",
    "axes[num_models].barh(combined_shap.head(12)['Feature'], combined_shap.head(12)['avg_shap'],\n",
    "             color='darkgreen', edgecolor='black', alpha=0.7)\n",
    "axes[num_models].set_xlabel('Average SHAP Importance', fontweight='bold')\n",
    "axes[num_models].set_title('CONSENSUS - Average SHAP Across All Base Models', fontweight='bold')\n",
    "axes[num_models].invert_yaxis()\n",
    "axes[num_models].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(num_models + 1, len(axes)):\n",
    "    axes[idx].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../saved_base_models/shap_analysis/base_models_shap_comparison.png', dpi=300, bbox_inches='tight')\n",
    "print(\"\\nSaved: ../saved_base_models/shap_analysis/base_models_shap_comparison.png\")\n",
    "plt.show()\n",
    "\n",
    "# Find the best tree-based model for detailed analysis\n",
    "tree_based_models = {k: v for k, v in shap_results.items() \n",
    "                     if 'random_forest' in k.lower() or 'decision_tree' in k.lower()}\n",
    "\n",
    "if tree_based_models:\n",
    "    # Select random forest if available, otherwise use decision tree\n",
    "    best_model_name = next((k for k in tree_based_models.keys() if 'random_forest' in k.lower()), \n",
    "                           next(iter(tree_based_models.keys())))\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(f\"DETAILED SHAP PLOTS FOR {best_model_name.upper()}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    best_shap_values = shap_results[best_model_name]['shap_values']\n",
    "    best_explainer = shap_results[best_model_name]['explainer']\n",
    "    \n",
    "    sample_size = min(100, len(best_shap_values))\n",
    "    \n",
    "    # Summary plot\n",
    "    print(f\"\\n1. SHAP Summary Plot (Feature Impact Distribution)\")\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(best_shap_values[:sample_size], \n",
    "                     X_test[:sample_size], \n",
    "                     feature_names=feature_names,\n",
    "                     show=False)\n",
    "    plt.title(f'SHAP Summary Plot - {best_model_name.upper()}', fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../saved_base_models/shap_analysis/{best_model_name}_summary_plot.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved: ../saved_base_models/shap_analysis/{best_model_name}_summary_plot.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Bar plot\n",
    "    print(f\"\\n2. SHAP Summary Plot (Bar)\")\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.summary_plot(best_shap_values[:sample_size], \n",
    "                     X_test[:sample_size], \n",
    "                     feature_names=feature_names,\n",
    "                     plot_type=\"bar\",\n",
    "                     show=False)\n",
    "    plt.title(f'SHAP Feature Importance - {best_model_name.upper()}', fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../saved_base_models/shap_analysis/{best_model_name}_bar_plot.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved: ../saved_base_models/shap_analysis/{best_model_name}_bar_plot.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Waterfall plot\n",
    "    print(f\"\\n3. SHAP Waterfall Plot (First Prediction)\")\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    shap.waterfall_plot(\n",
    "        shap.Explanation(values=best_shap_values[0],\n",
    "                        base_values=best_explainer.expected_value,\n",
    "                        data=X_test[0],\n",
    "                        feature_names=feature_names),\n",
    "        show=False\n",
    "    )\n",
    "    plt.title(f'SHAP Waterfall Plot - Sample Prediction ({best_model_name.upper()})', \n",
    "             fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../saved_base_models/shap_analysis/{best_model_name}_waterfall_plot.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved: ../saved_base_models/shap_analysis/{best_model_name}_waterfall_plot.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Dependence plots\n",
    "    top_features = combined_shap.head(3)['Feature'].tolist()\n",
    "    \n",
    "    print(f\"\\n4. SHAP Dependence Plots (Top 3 Features)\")\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    for idx, feature in enumerate(top_features):\n",
    "        feature_idx = feature_names.index(feature)\n",
    "        \n",
    "        shap.dependence_plot(\n",
    "            feature_idx,\n",
    "            best_shap_values[:sample_size],\n",
    "            X_test[:sample_size],\n",
    "            feature_names=feature_names,\n",
    "            ax=axes[idx],\n",
    "            show=False\n",
    "        )\n",
    "        axes[idx].set_title(f'SHAP Dependence: {feature}', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'../saved_base_models/shap_analysis/{best_model_name}_dependence_plots.png', dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved: ../saved_base_models/shap_analysis/{best_model_name}_dependence_plots.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Force plots\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"SHAP FORCE PLOTS FOR INDIVIDUAL PREDICTIONS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    sample_indices = [0, 1, 2]\n",
    "    \n",
    "    for sample_idx in sample_indices:\n",
    "        print(f\"\\nForce plot for sample {sample_idx}\")\n",
    "        plt.figure(figsize=(20, 3))\n",
    "        \n",
    "        shap.force_plot(\n",
    "            best_explainer.expected_value,\n",
    "            best_shap_values[sample_idx],\n",
    "            X_test[sample_idx],\n",
    "            feature_names=feature_names,\n",
    "            matplotlib=True,\n",
    "            show=False\n",
    "        )\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'../saved_base_models/shap_analysis/force_plot_sample_{sample_idx}.png', \n",
    "                    dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved: ../saved_base_models/shap_analysis/force_plot_sample_{sample_idx}.png\")\n",
    "        plt.show()\n",
    "\n",
    "# Save SHAP importance data\n",
    "for name, shap_data in shap_results.items():\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'SHAP_Importance': shap_data['feature_importance']\n",
    "    }).sort_values('SHAP_Importance', ascending=False)\n",
    "    \n",
    "    importance_df.to_csv(f'../saved_base_models/shap_analysis/{name}_shap_importance.csv', index=False)\n",
    "\n",
    "combined_shap.to_csv('../saved_base_models/shap_analysis/combined_shap_importance.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SHAP ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nSHAP importance data saved to: ../saved_base_models/shap_analysis/\")\n",
    "print(f\"\\nFiles created:\")\n",
    "for name in shap_results.keys():\n",
    "    print(f\"   {name}_shap_importance.csv\")\n",
    "print(f\"   combined_shap_importance.csv\")\n",
    "print(f\"   base_models_shap_comparison.png\")\n",
    "if tree_based_models:\n",
    "    print(f\"   {best_model_name}_summary_plot.png\")\n",
    "    print(f\"   {best_model_name}_bar_plot.png\")\n",
    "    print(f\"   {best_model_name}_waterfall_plot.png\")\n",
    "    print(f\"   {best_model_name}_dependence_plots.png\")\n",
    "    print(f\"   force_plot_sample_0.png, force_plot_sample_1.png, force_plot_sample_2.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
