{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba88d7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add parent directory so we can import preprocessor class\n",
    "models_path = os.path.abspath('..')\n",
    "if models_path not in sys.path:\n",
    "    sys.path.insert(0, models_path)\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from regression_preprocessor import RegressionPreprocessor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (16, 8)\n",
    "\n",
    "print(\"All libraries imported successfully\")\n",
    "\n",
    "# Cell 2: Load Unseen Data\n",
    "unseen_df = pd.read_csv('../data/unseen_data.csv')\n",
    "\n",
    "print(\"UNSEEN DATA OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Shape: {unseen_df.shape}\")\n",
    "print(f\"Columns: {list(unseen_df.columns)}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "print(unseen_df.head().to_string())\n",
    "\n",
    "print(f\"\\nMissing values:\")\n",
    "print(unseen_df.isnull().sum()[unseen_df.isnull().sum() > 0])\n",
    "\n",
    "print(f\"\\nTarget column (ExamResultPercent) stats:\")\n",
    "print(f\"   Mean: {unseen_df['ExamResultPercent'].mean():.2f}\")\n",
    "print(f\"   Std: {unseen_df['ExamResultPercent'].std():.2f}\")\n",
    "print(f\"   Min: {unseen_df['ExamResultPercent'].min():.2f}\")\n",
    "print(f\"   Max: {unseen_df['ExamResultPercent'].max():.2f}\")\n",
    "\n",
    "# Separate features and target\n",
    "y_unseen = unseen_df['ExamResultPercent'].values\n",
    "# Keep the full dataframe for preprocessing (preprocessor will handle column selection)\n",
    "print(f\"\\nTarget (y_unseen) shape: {y_unseen.shape}\")\n",
    "\n",
    "# Cell 3: Load Preprocessors\n",
    "# Dataset preprocessor (trained on raw data)\n",
    "dataset_preprocessor = joblib.load('../regression_processed_data_dataset/preprocessor.pkl')\n",
    "print(f\"Dataset Preprocessor loaded: {dataset_preprocessor}\")\n",
    "print(f\"   Feature count: {len(dataset_preprocessor.get_feature_names())}\")\n",
    "\n",
    "# Augmented preprocessor (trained on augmented data)\n",
    "augmented_preprocessor = joblib.load('../regression_processed_data/preprocessor.pkl')\n",
    "print(f\"Augmented Preprocessor loaded: {augmented_preprocessor}\")\n",
    "print(f\"   Feature count: {len(augmented_preprocessor.get_feature_names())}\")\n",
    "\n",
    "# Cell 4: Preprocess Unseen Data with Both Preprocessors\n",
    "# Preprocess for dataset models (raw data trained)\n",
    "print(\"PREPROCESSING UNSEEN DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    X_unseen_dataset = dataset_preprocessor.transform(unseen_df)\n",
    "    print(f\"Dataset preprocessor output shape: {X_unseen_dataset.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error with dataset preprocessor: {e}\")\n",
    "    # Fallback: try manual preprocessing\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "try:\n",
    "    X_unseen_augmented = augmented_preprocessor.transform(unseen_df)\n",
    "    print(f\"Augmented preprocessor output shape: {X_unseen_augmented.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error with augmented preprocessor: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\nPreprocessing complete for both model sets\")\n",
    "\n",
    "# Cell 5: Load All Dataset Models (Trained on Raw Data)\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING DATASET MODELS (TRAINED ON RAW DATA)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Base models trained on raw dataset\n",
    "dataset_base_models = {}\n",
    "base_model_names = ['knn', 'decision_tree', 'random_forest', 'svr', 'ridge', 'neural_network']\n",
    "\n",
    "for name in base_model_names:\n",
    "    model_path = f'../saved_base_models_dataset/{name}_model.pkl'\n",
    "    try:\n",
    "        dataset_base_models[name] = joblib.load(model_path)\n",
    "        print(f\"   [OK] Loaded: {name}_model.pkl\")\n",
    "    except Exception as e:\n",
    "        print(f\"   [ERROR] Failed to load {name}_model.pkl: {e}\")\n",
    "\n",
    "# Ensemble models trained on raw dataset\n",
    "dataset_ensemble_models = {}\n",
    "ensemble_model_names = [\n",
    "    'bagging_neural_network',\n",
    "    'bagging_random_forest',\n",
    "    'boosting_gradient_boost',\n",
    "    'stacking_neural_ridge_neural_final',\n",
    "    'stacking_neural_ridge_ridge_final',\n",
    "    'stacking_neural_only'\n",
    "]\n",
    "\n",
    "for name in ensemble_model_names:\n",
    "    model_path = f'../saved_ensemble_models_dataset/{name}_ensemble.pkl'\n",
    "    try:\n",
    "        with open(model_path, 'rb') as f:\n",
    "            dataset_ensemble_models[name] = pickle.load(f)\n",
    "        print(f\"   [OK] Loaded: {name}_ensemble.pkl\")\n",
    "    except Exception as e:\n",
    "        print(f\"   [ERROR] Failed to load {name}_ensemble.pkl: {e}\")\n",
    "\n",
    "print(f\"\\nDataset base models loaded: {len(dataset_base_models)}\")\n",
    "print(f\"Dataset ensemble models loaded: {len(dataset_ensemble_models)}\")\n",
    "\n",
    "# Cell 6: Load All Augmented Models (Trained on Augmented Data)\n",
    "print(\"=\" * 70)\n",
    "print(\"LOADING AUGMENTED MODELS (TRAINED ON AUGMENTED DATA)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Base models trained on augmented data\n",
    "augmented_base_models = {}\n",
    "for name in base_model_names:\n",
    "    model_path = f'../saved_base_models/{name}_model.pkl'\n",
    "    try:\n",
    "        augmented_base_models[name] = joblib.load(model_path)\n",
    "        print(f\"   [OK] Loaded: {name}_model.pkl\")\n",
    "    except Exception as e:\n",
    "        print(f\"   [ERROR] Failed to load {name}_model.pkl: {e}\")\n",
    "\n",
    "# Ensemble models trained on augmented data\n",
    "augmented_ensemble_models = {}\n",
    "for name in ensemble_model_names:\n",
    "    model_path = f'../saved_ensemble_models/{name}_ensemble.pkl'\n",
    "    try:\n",
    "        with open(model_path, 'rb') as f:\n",
    "            augmented_ensemble_models[name] = pickle.load(f)\n",
    "        print(f\"   [OK] Loaded: {name}_ensemble.pkl\")\n",
    "    except Exception as e:\n",
    "        print(f\"   [ERROR] Failed to load {name}_ensemble.pkl: {e}\")\n",
    "\n",
    "print(f\"\\nAugmented base models loaded: {len(augmented_base_models)}\")\n",
    "print(f\"Augmented ensemble models loaded: {len(augmented_ensemble_models)}\")\n",
    "\n",
    "# Cell 7: Predict with Dataset Models (Raw Data Trained) on Unseen Data\n",
    "print(\"=\" * 70)\n",
    "print(\"PREDICTIONS: DATASET MODELS (TRAINED ON RAW DATA)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "dataset_predictions = {}\n",
    "\n",
    "# Base model predictions\n",
    "print(\"\\n--- BASE MODELS ---\")\n",
    "for name, model in dataset_base_models.items():\n",
    "    try:\n",
    "        y_pred = model.predict(X_unseen_dataset)\n",
    "        \n",
    "        mse = mean_squared_error(y_unseen, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_unseen, y_pred)\n",
    "        r2 = r2_score(y_unseen, y_pred)\n",
    "        \n",
    "        dataset_predictions[f\"BASE_{name}\"] = {\n",
    "            'type': 'Base',\n",
    "            'model_name': name.upper(),\n",
    "            'training_data': 'Raw Dataset',\n",
    "            'y_pred': y_pred,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2': r2,\n",
    "            'mse': mse\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n   {name.upper()}:\")\n",
    "        print(f\"      RMSE: {rmse:.4f}\")\n",
    "        print(f\"      MAE:  {mae:.4f}\")\n",
    "        print(f\"      R2:   {r2:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   [ERROR] {name.upper()}: {e}\")\n",
    "\n",
    "# Ensemble model predictions\n",
    "print(\"\\n--- ENSEMBLE MODELS ---\")\n",
    "for name, model in dataset_ensemble_models.items():\n",
    "    try:\n",
    "        y_pred = model.predict(X_unseen_dataset)\n",
    "        \n",
    "        mse = mean_squared_error(y_unseen, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_unseen, y_pred)\n",
    "        r2 = r2_score(y_unseen, y_pred)\n",
    "        \n",
    "        dataset_predictions[f\"ENSEMBLE_{name}\"] = {\n",
    "            'type': 'Ensemble',\n",
    "            'model_name': name.replace('_', ' ').upper(),\n",
    "            'training_data': 'Raw Dataset',\n",
    "            'y_pred': y_pred,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2': r2,\n",
    "            'mse': mse\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n   {name.replace('_', ' ').upper()}:\")\n",
    "        print(f\"      RMSE: {rmse:.4f}\")\n",
    "        print(f\"      MAE:  {mae:.4f}\")\n",
    "        print(f\"      R2:   {r2:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   [ERROR] {name.upper()}: {e}\")\n",
    "\n",
    "\n",
    "# Cell 8: Predict with Augmented Models (Augmented Data Trained) on Unseen Data\n",
    "print(\"=\" * 70)\n",
    "print(\"PREDICTIONS: AUGMENTED MODELS (TRAINED ON AUGMENTED DATA)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "augmented_predictions = {}\n",
    "\n",
    "# Base model predictions\n",
    "print(\"\\n--- BASE MODELS ---\")\n",
    "for name, model in augmented_base_models.items():\n",
    "    try:\n",
    "        y_pred = model.predict(X_unseen_augmented)\n",
    "        \n",
    "        mse = mean_squared_error(y_unseen, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_unseen, y_pred)\n",
    "        r2 = r2_score(y_unseen, y_pred)\n",
    "        \n",
    "        augmented_predictions[f\"BASE_{name}\"] = {\n",
    "            'type': 'Base',\n",
    "            'model_name': name.upper(),\n",
    "            'training_data': 'Augmented',\n",
    "            'y_pred': y_pred,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2': r2,\n",
    "            'mse': mse\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n   {name.upper()}:\")\n",
    "        print(f\"      RMSE: {rmse:.4f}\")\n",
    "        print(f\"      MAE:  {mae:.4f}\")\n",
    "        print(f\"      R2:   {r2:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   [ERROR] {name.upper()}: {e}\")\n",
    "\n",
    "# Ensemble model predictions\n",
    "print(\"\\n--- ENSEMBLE MODELS ---\")\n",
    "for name, model in augmented_ensemble_models.items():\n",
    "    try:\n",
    "        y_pred = model.predict(X_unseen_augmented)\n",
    "        \n",
    "        mse = mean_squared_error(y_unseen, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_unseen, y_pred)\n",
    "        r2 = r2_score(y_unseen, y_pred)\n",
    "        \n",
    "        augmented_predictions[f\"ENSEMBLE_{name}\"] = {\n",
    "            'type': 'Ensemble',\n",
    "            'model_name': name.replace('_', ' ').upper(),\n",
    "            'training_data': 'Augmented',\n",
    "            'y_pred': y_pred,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae,\n",
    "            'r2': r2,\n",
    "            'mse': mse\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n   {name.replace('_', ' ').upper()}:\")\n",
    "        print(f\"      RMSE: {rmse:.4f}\")\n",
    "        print(f\"      MAE:  {mae:.4f}\")\n",
    "        print(f\"      R2:   {r2:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   [ERROR] {name.upper()}: {e}\")\n",
    "\n",
    "# Cell 9: Combined Summary Table - All Models\n",
    "print(\"=\" * 110)\n",
    "print(\"COMPLETE UNSEEN DATA PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 110)\n",
    "\n",
    "all_results = []\n",
    "\n",
    "# Add dataset model results\n",
    "for key, data in dataset_predictions.items():\n",
    "    all_results.append({\n",
    "        'Training Data': data['training_data'],\n",
    "        'Type': data['type'],\n",
    "        'Model': data['model_name'],\n",
    "        'RMSE': data['rmse'],\n",
    "        'MAE': data['mae'],\n",
    "        'R2': data['r2'],\n",
    "        'MSE': data['mse']\n",
    "    })\n",
    "\n",
    "# Add augmented model results\n",
    "for key, data in augmented_predictions.items():\n",
    "    all_results.append({\n",
    "        'Training Data': data['training_data'],\n",
    "        'Type': data['type'],\n",
    "        'Model': data['model_name'],\n",
    "        'RMSE': data['rmse'],\n",
    "        'MAE': data['mae'],\n",
    "        'R2': data['r2'],\n",
    "        'MSE': data['mse']\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "results_df = results_df.sort_values('RMSE')\n",
    "\n",
    "# Format for display\n",
    "display_df = results_df.copy()\n",
    "display_df['RMSE'] = display_df['RMSE'].apply(lambda x: f\"{x:.4f}\")\n",
    "display_df['MAE'] = display_df['MAE'].apply(lambda x: f\"{x:.4f}\")\n",
    "display_df['R2'] = display_df['R2'].apply(lambda x: f\"{x:.4f}\")\n",
    "display_df['MSE'] = display_df['MSE'].apply(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "print(\"\\nALL MODELS RANKED BY RMSE (BEST TO WORST):\")\n",
    "print(display_df.to_string(index=False))\n",
    "\n",
    "# Best overall\n",
    "best = results_df.iloc[0]\n",
    "print(f\"\\nBest Overall Model on Unseen Data:\")\n",
    "print(f\"   Model: {best['Model']}\")\n",
    "print(f\"   Training Data: {best['Training Data']}\")\n",
    "print(f\"   Type: {best['Type']}\")\n",
    "print(f\"   RMSE: {best['RMSE']:.4f}\")\n",
    "print(f\"   MAE: {best['MAE']:.4f}\")\n",
    "print(f\"   R2: {best['R2']:.4f}\")\n",
    "\n",
    "# Cell 10: Side-by-Side Comparison - Raw vs Augmented for Each Model\n",
    "print(\"=\" * 110)\n",
    "print(\"SIDE-BY-SIDE COMPARISON: RAW DATASET vs AUGMENTED TRAINING DATA\")\n",
    "print(\"=\" * 110)\n",
    "\n",
    "comparison_rows = []\n",
    "\n",
    "# Compare base models\n",
    "for name in base_model_names:\n",
    "    dataset_key = f\"BASE_{name}\"\n",
    "    augmented_key = f\"BASE_{name}\"\n",
    "    \n",
    "    if dataset_key in dataset_predictions and augmented_key in augmented_predictions:\n",
    "        d = dataset_predictions[dataset_key]\n",
    "        a = augmented_predictions[augmented_key]\n",
    "        \n",
    "        rmse_diff = d['rmse'] - a['rmse']\n",
    "        mae_diff = d['mae'] - a['mae']\n",
    "        r2_diff = a['r2'] - d['r2']\n",
    "        \n",
    "        comparison_rows.append({\n",
    "            'Type': 'Base',\n",
    "            'Model': name.upper(),\n",
    "            'Raw RMSE': d['rmse'],\n",
    "            'Aug RMSE': a['rmse'],\n",
    "            'RMSE Diff': rmse_diff,\n",
    "            'Raw MAE': d['mae'],\n",
    "            'Aug MAE': a['mae'],\n",
    "            'MAE Diff': mae_diff,\n",
    "            'Raw R2': d['r2'],\n",
    "            'Aug R2': a['r2'],\n",
    "            'R2 Diff': r2_diff,\n",
    "            'Better': 'Augmented' if a['rmse'] < d['rmse'] else 'Raw Dataset'\n",
    "        })\n",
    "\n",
    "# Compare ensemble models\n",
    "for name in ensemble_model_names:\n",
    "    dataset_key = f\"ENSEMBLE_{name}\"\n",
    "    augmented_key = f\"ENSEMBLE_{name}\"\n",
    "    \n",
    "    if dataset_key in dataset_predictions and augmented_key in augmented_predictions:\n",
    "        d = dataset_predictions[dataset_key]\n",
    "        a = augmented_predictions[augmented_key]\n",
    "        \n",
    "        rmse_diff = d['rmse'] - a['rmse']\n",
    "        mae_diff = d['mae'] - a['mae']\n",
    "        r2_diff = a['r2'] - d['r2']\n",
    "        \n",
    "        comparison_rows.append({\n",
    "            'Type': 'Ensemble',\n",
    "            'Model': name.replace('_', ' ').upper(),\n",
    "            'Raw RMSE': d['rmse'],\n",
    "            'Aug RMSE': a['rmse'],\n",
    "            'RMSE Diff': rmse_diff,\n",
    "            'Raw MAE': d['mae'],\n",
    "            'Aug MAE': a['mae'],\n",
    "            'MAE Diff': mae_diff,\n",
    "            'Raw R2': d['r2'],\n",
    "            'Aug R2': a['r2'],\n",
    "            'R2 Diff': r2_diff,\n",
    "            'Better': 'Augmented' if a['rmse'] < d['rmse'] else 'Raw Dataset'\n",
    "        })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_rows)\n",
    "\n",
    "# Display formatted\n",
    "print(\"\\n\" + comparison_df.to_string(index=False, float_format='{:.4f}'.format))\n",
    "\n",
    "# Count which training approach wins\n",
    "raw_wins = sum(1 for r in comparison_rows if r['Better'] == 'Raw Dataset')\n",
    "aug_wins = sum(1 for r in comparison_rows if r['Better'] == 'Augmented')\n",
    "print(f\"\\nRaw Dataset wins: {raw_wins} models\")\n",
    "print(f\"Augmented wins: {aug_wins} models\")\n",
    "\n",
    "# NOTE: Positive RMSE Diff means Raw is worse (higher RMSE), Augmented is better\n",
    "# NOTE: Positive R2 Diff means Augmented is better (higher R2)\n",
    "print(\"\\nNote: RMSE Diff = Raw - Augmented (positive means Augmented is better)\")\n",
    "print(\"Note: R2 Diff = Augmented - Raw (positive means Augmented is better)\")\n",
    "\n",
    "# Cell 11: Visualization - RMSE Comparison Bar Chart\n",
    "fig, axes = plt.subplots(2, 2, figsize=(22, 16))\n",
    "\n",
    "# Plot 1: Base Models - Raw vs Augmented RMSE\n",
    "base_comparison = [r for r in comparison_rows if r['Type'] == 'Base']\n",
    "if base_comparison:\n",
    "    base_names = [r['Model'] for r in base_comparison]\n",
    "    raw_rmse = [r['Raw RMSE'] for r in base_comparison]\n",
    "    aug_rmse = [r['Aug RMSE'] for r in base_comparison]\n",
    "    \n",
    "    x = np.arange(len(base_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = axes[0, 0].bar(x - width/2, raw_rmse, width, label='Raw Dataset', \n",
    "                           color='#3498db', alpha=0.8, edgecolor='black')\n",
    "    bars2 = axes[0, 0].bar(x + width/2, aug_rmse, width, label='Augmented', \n",
    "                           color='#e74c3c', alpha=0.8, edgecolor='black')\n",
    "    \n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        axes[0, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        axes[0, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{height:.3f}', ha='center', va='bottom', fontsize=8)\n",
    "    \n",
    "    axes[0, 0].set_xlabel('Base Models', fontweight='bold', fontsize=12)\n",
    "    axes[0, 0].set_ylabel('RMSE', fontweight='bold', fontsize=12)\n",
    "    axes[0, 0].set_title('Base Models: Raw vs Augmented RMSE on Unseen Data', fontweight='bold', fontsize=13)\n",
    "    axes[0, 0].set_xticks(x)\n",
    "    axes[0, 0].set_xticklabels(base_names, rotation=30, ha='right', fontsize=9)\n",
    "    axes[0, 0].legend(fontsize=10)\n",
    "    axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Ensemble Models - Raw vs Augmented RMSE\n",
    "ensemble_comparison = [r for r in comparison_rows if r['Type'] == 'Ensemble']\n",
    "if ensemble_comparison:\n",
    "    ens_names = [r['Model'][:25] for r in ensemble_comparison]\n",
    "    raw_rmse_ens = [r['Raw RMSE'] for r in ensemble_comparison]\n",
    "    aug_rmse_ens = [r['Aug RMSE'] for r in ensemble_comparison]\n",
    "    \n",
    "    x_ens = np.arange(len(ens_names))\n",
    "    \n",
    "    bars1 = axes[0, 1].bar(x_ens - width/2, raw_rmse_ens, width, label='Raw Dataset', \n",
    "                           color='#3498db', alpha=0.8, edgecolor='black')\n",
    "    bars2 = axes[0, 1].bar(x_ens + width/2, aug_rmse_ens, width, label='Augmented', \n",
    "                           color='#e74c3c', alpha=0.8, edgecolor='black')\n",
    "    \n",
    "    for bar in bars1:\n",
    "        height = bar.get_height()\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{height:.3f}', ha='center', va='bottom', fontsize=7)\n",
    "    for bar in bars2:\n",
    "        height = bar.get_height()\n",
    "        axes[0, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{height:.3f}', ha='center', va='bottom', fontsize=7)\n",
    "    \n",
    "    axes[0, 1].set_xlabel('Ensemble Models', fontweight='bold', fontsize=12)\n",
    "    axes[0, 1].set_ylabel('RMSE', fontweight='bold', fontsize=12)\n",
    "    axes[0, 1].set_title('Ensemble Models: Raw vs Augmented RMSE on Unseen Data', fontweight='bold', fontsize=13)\n",
    "    axes[0, 1].set_xticks(x_ens)\n",
    "    axes[0, 1].set_xticklabels(ens_names, rotation=35, ha='right', fontsize=7)\n",
    "    axes[0, 1].legend(fontsize=10)\n",
    "    axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 3: R2 Comparison - All Models\n",
    "all_model_names = [r['Model'][:20] for r in comparison_rows]\n",
    "raw_r2_all = [r['Raw R2'] for r in comparison_rows]\n",
    "aug_r2_all = [r['Aug R2'] for r in comparison_rows]\n",
    "\n",
    "x_all = np.arange(len(all_model_names))\n",
    "width_all = 0.35\n",
    "\n",
    "bars1 = axes[1, 0].bar(x_all - width_all/2, raw_r2_all, width_all, label='Raw Dataset', \n",
    "                       color='#3498db', alpha=0.8, edgecolor='black')\n",
    "bars2 = axes[1, 0].bar(x_all + width_all/2, aug_r2_all, width_all, label='Augmented', \n",
    "                       color='#e74c3c', alpha=0.8, edgecolor='black')\n",
    "\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    axes[1, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}', ha='center', va='bottom', fontsize=6)\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    axes[1, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.3f}', ha='center', va='bottom', fontsize=6)\n",
    "\n",
    "axes[1, 0].set_xlabel('All Models', fontweight='bold', fontsize=12)\n",
    "axes[1, 0].set_ylabel('R2 Score', fontweight='bold', fontsize=12)\n",
    "axes[1, 0].set_title('All Models: Raw vs Augmented R2 on Unseen Data', fontweight='bold', fontsize=13)\n",
    "axes[1, 0].set_xticks(x_all)\n",
    "axes[1, 0].set_xticklabels(all_model_names, rotation=45, ha='right', fontsize=7)\n",
    "axes[1, 0].legend(fontsize=10)\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 4: RMSE Difference (Raw - Augmented)\n",
    "rmse_diffs = [r['RMSE Diff'] for r in comparison_rows]\n",
    "bar_colors = ['#2ecc71' if d > 0 else '#e74c3c' for d in rmse_diffs]\n",
    "\n",
    "bars = axes[1, 1].bar(all_model_names, rmse_diffs, color=bar_colors, alpha=0.8, edgecolor='black')\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    va = 'bottom' if height >= 0 else 'top'\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.4f}', ha='center', va=va, fontsize=7)\n",
    "\n",
    "axes[1, 1].axhline(y=0, color='black', linewidth=1)\n",
    "axes[1, 1].set_xlabel('Models', fontweight='bold', fontsize=12)\n",
    "axes[1, 1].set_ylabel('RMSE Difference (Raw - Augmented)', fontweight='bold', fontsize=12)\n",
    "axes[1, 1].set_title('RMSE Difference: Positive = Augmented Better', fontweight='bold', fontsize=13)\n",
    "axes[1, 1].set_xticklabels(all_model_names, rotation=45, ha='right', fontsize=7)\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cell 12: Detailed Per-Sample Predictions for Best Models\n",
    "print(\"=\" * 70)\n",
    "print(\"PER-SAMPLE PREDICTION DETAILS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Find best model from each training approach\n",
    "best_dataset_key = min(dataset_predictions.items(), key=lambda x: x[1]['rmse'])\n",
    "best_augmented_key = min(augmented_predictions.items(), key=lambda x: x[1]['rmse'])\n",
    "\n",
    "print(f\"\\nBest Raw Dataset Model: {best_dataset_key[1]['model_name']}\")\n",
    "print(f\"Best Augmented Model: {best_augmented_key[1]['model_name']}\")\n",
    "\n",
    "# Build per-sample comparison dataframe\n",
    "per_sample_df = pd.DataFrame({\n",
    "    'Sample': range(1, len(y_unseen) + 1),\n",
    "    'Actual': y_unseen,\n",
    "    f'Pred_Raw_{best_dataset_key[1][\"model_name\"]}': best_dataset_key[1]['y_pred'],\n",
    "    f'Pred_Aug_{best_augmented_key[1][\"model_name\"]}': best_augmented_key[1]['y_pred'],\n",
    "    f'Error_Raw': np.abs(y_unseen - best_dataset_key[1]['y_pred']),\n",
    "    f'Error_Aug': np.abs(y_unseen - best_augmented_key[1]['y_pred']),\n",
    "})\n",
    "\n",
    "per_sample_df['Better'] = per_sample_df.apply(\n",
    "    lambda row: 'Raw' if row['Error_Raw'] < row['Error_Aug'] else 'Augmented', axis=1\n",
    ")\n",
    "\n",
    "print(f\"\\nPer-Sample Predictions (Best Models):\")\n",
    "print(per_sample_df.to_string(index=False, float_format='{:.4f}'.format))\n",
    "\n",
    "raw_better_count = (per_sample_df['Better'] == 'Raw').sum()\n",
    "aug_better_count = (per_sample_df['Better'] == 'Augmented').sum()\n",
    "print(f\"\\nRaw Dataset closer: {raw_better_count}/{len(y_unseen)} samples\")\n",
    "print(f\"Augmented closer: {aug_better_count}/{len(y_unseen)} samples\")\n",
    "\n",
    "# Cell 13: Prediction vs Actual Scatter Plots (Top 4 Models)\n",
    "# Get top 4 models overall by RMSE\n",
    "all_predictions = {}\n",
    "for k, v in dataset_predictions.items():\n",
    "    all_predictions[f\"Raw_{k}\"] = v\n",
    "for k, v in augmented_predictions.items():\n",
    "    all_predictions[f\"Aug_{k}\"] = v\n",
    "\n",
    "sorted_all = sorted(all_predictions.items(), key=lambda x: x[1]['rmse'])\n",
    "top_4 = sorted_all[:4]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "axes_flat = axes.flatten()\n",
    "colors_scatter = ['#2ecc71', '#3498db', '#e74c3c', '#9b59b6']\n",
    "\n",
    "for idx, (key, data) in enumerate(top_4):\n",
    "    ax = axes_flat[idx]\n",
    "    y_pred = data['y_pred']\n",
    "    \n",
    "    ax.scatter(y_unseen, y_pred, alpha=0.7, s=80, color=colors_scatter[idx], edgecolor='black')\n",
    "    \n",
    "    min_val = min(y_unseen.min(), y_pred.min())\n",
    "    max_val = max(y_unseen.max(), y_pred.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    \n",
    "    ax.set_xlabel('Actual ExamResultPercent', fontweight='bold', fontsize=11)\n",
    "    ax.set_ylabel('Predicted ExamResultPercent', fontweight='bold', fontsize=11)\n",
    "    ax.set_title(f\"{data['training_data']} - {data['model_name']}\\n\"\n",
    "                 f\"RMSE: {data['rmse']:.4f}, R2: {data['r2']:.4f}\", \n",
    "                 fontweight='bold', fontsize=11)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Top 4 Models: Predicted vs Actual on Unseen Data', fontsize=15, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cell 14: Residual Analysis for Top Models\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "for idx, (key, data) in enumerate(top_4):\n",
    "    ax = axes_flat[idx]\n",
    "    residuals = y_unseen - data['y_pred']\n",
    "    \n",
    "    ax.scatter(data['y_pred'], residuals, alpha=0.7, s=80, \n",
    "              color=colors_scatter[idx], edgecolor='black')\n",
    "    ax.axhline(y=0, color='red', linewidth=2, linestyle='--')\n",
    "    \n",
    "    ax.set_xlabel('Predicted Value', fontweight='bold', fontsize=11)\n",
    "    ax.set_ylabel('Residual (Actual - Predicted)', fontweight='bold', fontsize=11)\n",
    "    ax.set_title(f\"{data['training_data']} - {data['model_name']}\\n\"\n",
    "                 f\"Mean Residual: {np.mean(residuals):.4f}, Std: {np.std(residuals):.4f}\",\n",
    "                 fontweight='bold', fontsize=11)\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.suptitle('Top 4 Models: Residual Plots on Unseen Data', fontsize=15, fontweight='bold', y=1.01)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cell 15: Error Distribution Comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "\n",
    "# All raw dataset model errors\n",
    "raw_errors = {}\n",
    "for key, data in dataset_predictions.items():\n",
    "    model_label = f\"{data['type']}_{data['model_name']}\"[:25]\n",
    "    raw_errors[model_label] = np.abs(y_unseen - data['y_pred'])\n",
    "\n",
    "# All augmented model errors\n",
    "aug_errors = {}\n",
    "for key, data in augmented_predictions.items():\n",
    "    model_label = f\"{data['type']}_{data['model_name']}\"[:25]\n",
    "    aug_errors[model_label] = np.abs(y_unseen - data['y_pred'])\n",
    "\n",
    "# Box plot for raw dataset models\n",
    "raw_error_df = pd.DataFrame(raw_errors)\n",
    "raw_error_df.boxplot(ax=axes[0], rot=45, grid=True)\n",
    "axes[0].set_title('Raw Dataset Models - Absolute Error Distribution', fontweight='bold', fontsize=12)\n",
    "axes[0].set_ylabel('Absolute Error', fontweight='bold')\n",
    "axes[0].tick_params(axis='x', labelsize=7)\n",
    "\n",
    "# Box plot for augmented models\n",
    "aug_error_df = pd.DataFrame(aug_errors)\n",
    "aug_error_df.boxplot(ax=axes[1], rot=45, grid=True)\n",
    "axes[1].set_title('Augmented Models - Absolute Error Distribution', fontweight='bold', fontsize=12)\n",
    "axes[1].set_ylabel('Absolute Error', fontweight='bold')\n",
    "axes[1].tick_params(axis='x', labelsize=7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Cell 16: Save All Results to CSV\n",
    "os.makedirs('../unseen_data_results', exist_ok=True)\n",
    "\n",
    "# Save summary results\n",
    "results_df.to_csv('../unseen_data_results/unseen_data_all_results.csv', index=False)\n",
    "print(\"Saved: models/unseen_data_results/unseen_data_all_results.csv\")\n",
    "\n",
    "# Save comparison table\n",
    "comparison_df.to_csv('../unseen_data_results/raw_vs_augmented_comparison.csv', index=False)\n",
    "print(\"Saved: models/unseen_data_results/raw_vs_augmented_comparison.csv\")\n",
    "\n",
    "# Save per-sample predictions for all models\n",
    "all_sample_predictions = pd.DataFrame({'Actual': y_unseen})\n",
    "\n",
    "for key, data in dataset_predictions.items():\n",
    "    col_name = f\"Raw_{data['type']}_{data['model_name']}\"\n",
    "    all_sample_predictions[col_name] = data['y_pred']\n",
    "\n",
    "for key, data in augmented_predictions.items():\n",
    "    col_name = f\"Aug_{data['type']}_{data['model_name']}\"\n",
    "    all_sample_predictions[col_name] = data['y_pred']\n",
    "\n",
    "all_sample_predictions.to_csv('../unseen_data_results/unseen_data_all_predictions.csv', index=False)\n",
    "print(\"Saved: models/unseen_data_results/unseen_data_all_predictions.csv\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"UNSEEN DATA TESTING COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Total models tested: {len(dataset_predictions) + len(augmented_predictions)}\")\n",
    "print(f\"   Raw Dataset models: {len(dataset_predictions)}\")\n",
    "print(f\"   Augmented models: {len(augmented_predictions)}\")\n",
    "print(f\"Unseen data samples: {len(y_unseen)}\")\n",
    "print(f\"Results saved to: models/unseen_data_results/\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
