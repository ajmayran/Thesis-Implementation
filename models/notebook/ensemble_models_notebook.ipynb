{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f25c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully\n",
      "Data Loaded:\n",
      "   Training: (888, 16), Test: (222, 16)\n",
      "   Features: 16\n",
      "Base models loaded: ['knn', 'decision_tree', 'random_forest', 'svr', 'ridge', 'neural_network']\n",
      "Ensemble Models Created:\n",
      "   BAGGING NEURAL NETWORK\n",
      "   BAGGING RANDOM FOREST\n",
      "   BOOSTING GRADIENT BOOST\n",
      "   STACKING NEURAL RIDGE NEURAL FINAL\n",
      "   STACKING NEURAL RIDGE RIDGE FINAL\n",
      "   STACKING NEURAL ONLY\n",
      "\n",
      "NOTE: Bagging applied to Neural Network with 10 estimators\n",
      "NOTE: Stacking with Neural Network final estimator\n",
      "NOTE: Stacking with Ridge final estimator\n",
      "NOTE: Neural-only stacking configuration\n",
      "\n",
      "======================================================================\n",
      "TRAINING ENSEMBLE MODELS WITH 10-FOLD CROSS-VALIDATION\n",
      "======================================================================\n",
      "\n",
      "[MODEL] Training BAGGING_NEURAL_NETWORK with 10-fold CV...\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "from sklearn.ensemble import BaggingRegressor, GradientBoostingRegressor, StackingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"All libraries imported successfully\")\n",
    "\n",
    "# Cell 2: Load Data and Base Models\n",
    "X_train = np.load('../regression_processed_data/X_train.npy')\n",
    "X_test = np.load('../regression_processed_data/X_test.npy')\n",
    "y_train = np.load('../regression_processed_data/y_train.npy')\n",
    "y_test = np.load('../regression_processed_data/y_test.npy')\n",
    "\n",
    "# Load feature names for feature importance analysis\n",
    "with open('../regression_processed_data/feature_names.json', 'r') as f:\n",
    "    feature_names = json.load(f)\n",
    "\n",
    "# Load base models\n",
    "base_models = {}\n",
    "for model_name in ['knn', 'decision_tree', 'random_forest', 'svr', 'ridge', 'neural_network']:\n",
    "    base_models[model_name] = joblib.load(f'../saved_base_models/{model_name}_model.pkl')\n",
    "\n",
    "print(f\"Data Loaded:\")\n",
    "print(f\"   Training: {X_train.shape}, Test: {X_test.shape}\")\n",
    "print(f\"   Features: {len(feature_names)}\")\n",
    "print(f\"Base models loaded: {list(base_models.keys())}\")\n",
    "\n",
    "# Cell 3: Create Ensemble Models\n",
    "# Neural network for bagging base estimator\n",
    "neural_bagging_base = MLPRegressor(\n",
    "    hidden_layer_sizes=(100, 50),\n",
    "    activation='relu',\n",
    "    alpha=0.001,\n",
    "    learning_rate='adaptive',\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    early_stopping=True,\n",
    "    validation_fraction=0.1\n",
    ")\n",
    "\n",
    "# Neural network for final estimator in stacking\n",
    "neural_final_estimator = MLPRegressor(\n",
    "    hidden_layer_sizes=(100, 50),\n",
    "    activation='relu',\n",
    "    alpha=0.001,\n",
    "    learning_rate='adaptive',\n",
    "    random_state=42,\n",
    "    max_iter=1000,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Ridge regression for final estimator in stacking\n",
    "ridge_final_estimator = Ridge(\n",
    "    alpha=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "ensemble_models = {\n",
    "    'bagging_neural_network': BaggingRegressor(\n",
    "        estimator=neural_bagging_base,\n",
    "        n_estimators=10,\n",
    "        max_samples=0.8,\n",
    "        max_features=0.8,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'bagging_random_forest': RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        min_samples_split=5,\n",
    "        random_state=42,\n",
    "        bootstrap=True\n",
    "    ),\n",
    "    'boosting_gradient_boost': GradientBoostingRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=3,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'stacking_neural_ridge_neural_final': StackingRegressor(\n",
    "        estimators=[\n",
    "            ('neural_network', base_models['neural_network']),\n",
    "            ('ridge', base_models['ridge'])\n",
    "        ],\n",
    "        final_estimator=neural_final_estimator,\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'stacking_neural_ridge_ridge_final': StackingRegressor(\n",
    "        estimators=[\n",
    "            ('neural_network', base_models['neural_network']),\n",
    "            ('ridge', base_models['ridge'])\n",
    "        ],\n",
    "        final_estimator=ridge_final_estimator,\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    ),\n",
    "    'stacking_neural_only': StackingRegressor(\n",
    "        estimators=[\n",
    "            ('neural_network_1', base_models['neural_network'])\n",
    "        ],\n",
    "        final_estimator=MLPRegressor(\n",
    "            hidden_layer_sizes=(50,),\n",
    "            activation='relu',\n",
    "            alpha=0.001,\n",
    "            learning_rate='adaptive',\n",
    "            random_state=42,\n",
    "            max_iter=1000,\n",
    "            early_stopping=True\n",
    "        ),\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"Ensemble Models Created:\")\n",
    "for name in ensemble_models.keys():\n",
    "    print(f\"   {name.replace('_', ' ').upper()}\")\n",
    "\n",
    "print(\"\\nNOTE: Bagging applied to Neural Network with 10 estimators\")\n",
    "print(\"NOTE: Stacking with Neural Network final estimator\")\n",
    "print(\"NOTE: Stacking with Ridge final estimator\")\n",
    "print(\"NOTE: Neural-only stacking configuration\")\n",
    "\n",
    "# Cell 4: Train Ensemble Models with 10-Fold CV\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "ensemble_results = {}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING ENSEMBLE MODELS WITH 10-FOLD CROSS-VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, model in ensemble_models.items():\n",
    "    print(f\"\\n[MODEL] Training {name.upper()} with 10-fold CV...\")\n",
    "    \n",
    "    # Perform 10-fold CV\n",
    "    fold_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    \n",
    "    cv_mse_scores = -fold_scores\n",
    "    cv_mean = np.mean(cv_mse_scores)\n",
    "    cv_std = np.std(cv_mse_scores)\n",
    "    cv_rmse = np.sqrt(cv_mean)\n",
    "    \n",
    "    # Train on full training set\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Test predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    test_mse = mean_squared_error(y_test, y_pred)\n",
    "    test_rmse = np.sqrt(test_mse)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred)\n",
    "    test_r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    ensemble_results[name] = {\n",
    "        'model': model,\n",
    "        'cv_mse_mean': cv_mean,\n",
    "        'cv_mse_std': cv_std,\n",
    "        'cv_rmse': cv_rmse,\n",
    "        'cv_10fold_scores': fold_scores.tolist(),\n",
    "        'test_mse': test_mse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mae': test_mae,\n",
    "        'test_r2': test_r2,\n",
    "        'y_pred': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"   {name.replace('_', ' ').upper()} Trained\")\n",
    "    print(f\"      10-Fold CV RMSE: {cv_rmse:.4f}\")\n",
    "    print(f\"      Test RMSE: {test_rmse:.4f}\")\n",
    "    print(f\"      Test R²: {test_r2:.4f}\")\n",
    "\n",
    "# Cell 5: Compare Ensemble Models\n",
    "summary_data = []\n",
    "for name, data in ensemble_results.items():\n",
    "    summary_data.append({\n",
    "        'Model': name.replace('_', ' ').upper(),\n",
    "        'CV RMSE': f\"{data['cv_rmse']:.4f}\",\n",
    "        'Test RMSE': f\"{data['test_rmse']:.4f}\",\n",
    "        'Test MAE': f\"{data['test_mae']:.4f}\",\n",
    "        'Test R²': f\"{data['test_r2']:.4f}\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ENSEMBLE MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "print(summary_df.to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DETAILED ENSEMBLE TEST RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sorted_models = sorted(ensemble_results.items(), key=lambda x: x[1]['test_rmse'])\n",
    "\n",
    "print(f\"\\n{'Rank':<5} {'Model':<50} {'Test RMSE':<12} {'CV RMSE':<12} {'R²':<10}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for rank, (name, data) in enumerate(sorted_models, 1):\n",
    "    print(f\"{rank:<5} {name.replace('_', ' ').upper():<50} \"\n",
    "          f\"{data['test_rmse']:<12.4f} {data['cv_rmse']:<12.4f} {data['test_r2']:<10.4f}\")\n",
    "\n",
    "best_model = sorted_models[0]\n",
    "print(f\"\\nBest Ensemble Model: {best_model[0].replace('_', ' ').upper()}\")\n",
    "print(f\"   Test RMSE: {best_model[1]['test_rmse']:.4f}\")\n",
    "print(f\"   Test R²: {best_model[1]['test_r2']:.4f}\")\n",
    "print(f\"   10-Fold CV RMSE: {best_model[1]['cv_rmse']:.4f}\")\n",
    "\n",
    "# Cell 5.5: Detailed Ensemble Predictions Analysis\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ENSEMBLE TEST PREDICTIONS ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "os.makedirs('../saved_ensemble_models', exist_ok=True)\n",
    "\n",
    "for name, data in ensemble_results.items():\n",
    "    y_pred = data['y_pred']\n",
    "    \n",
    "    prediction_details = pd.DataFrame({\n",
    "        'Actual': y_test,\n",
    "        'Predicted': y_pred,\n",
    "        'Error': y_test - y_pred,\n",
    "        'Absolute_Error': np.abs(y_test - y_pred),\n",
    "        'Percent_Error': np.abs((y_test - y_pred) / y_test) * 100\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{name.replace('_', ' ').upper()}\")\n",
    "    print(f\"   Total Test Samples: {len(y_test)}\")\n",
    "    print(f\"   Test RMSE: {data['test_rmse']:.4f}\")\n",
    "    print(f\"   Test MAE:  {data['test_mae']:.4f}\")\n",
    "    print(f\"   Test R²:   {data['test_r2']:.4f}\")\n",
    "    print(f\"   10-Fold CV RMSE: {data['cv_rmse']:.4f}\")\n",
    "    \n",
    "    residuals = y_test - y_pred\n",
    "    print(f\"   Mean Residual: {np.mean(residuals):.4f}\")\n",
    "    print(f\"   Std Residual:  {np.std(residuals):.4f}\")\n",
    "    \n",
    "    print(f\"\\n   Error Statistics:\")\n",
    "    print(f\"   Mean Absolute Error: {prediction_details['Absolute_Error'].mean():.4f}\")\n",
    "    print(f\"   Median Absolute Error: {prediction_details['Absolute_Error'].median():.4f}\")\n",
    "    print(f\"   Max Error: {prediction_details['Absolute_Error'].max():.4f}\")\n",
    "    print(f\"   Min Error: {prediction_details['Absolute_Error'].min():.4f}\")\n",
    "    \n",
    "    print(f\"\\n   First 10 predictions:\")\n",
    "    print(prediction_details.head(10).to_string(index=False))\n",
    "    \n",
    "    large_errors = prediction_details[prediction_details['Absolute_Error'] > data['test_rmse']]\n",
    "    if len(large_errors) > 0:\n",
    "        print(f\"\\n   Samples with error > RMSE ({len(large_errors)} samples):\")\n",
    "        print(large_errors.head(5).to_string(index=False))\n",
    "    \n",
    "    csv_file = f'../saved_ensemble_models/{name}_test_predictions.csv'\n",
    "    prediction_details.to_csv(csv_file, index=False)\n",
    "    print(f\"\\n   Saved detailed predictions: {csv_file}\")\n",
    "\n",
    "# Cell 6: Feature Importance Analysis for Ensemble Models\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ENSEMBLE FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ensemble_feature_importance = {}\n",
    "\n",
    "for name, data in ensemble_results.items():\n",
    "    model = data['model']\n",
    "    \n",
    "    print(f\"\\n{name.replace('_', ' ').upper()}:\")\n",
    "    \n",
    "    # Check if model has native feature importance\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importance = model.feature_importances_\n",
    "        \n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': importance\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        ensemble_feature_importance[name] = {\n",
    "            'method': 'native',\n",
    "            'importance': feature_importance_df.to_dict('records')\n",
    "        }\n",
    "        \n",
    "        print(f\"   Method: Native Feature Importance\")\n",
    "        print(f\"   Top 10 Features:\")\n",
    "        print(feature_importance_df.head(10).to_string(index=False))\n",
    "        \n",
    "    else:\n",
    "        # Use permutation importance for models without native feature importance\n",
    "        print(f\"   Calculating permutation importance...\")\n",
    "        \n",
    "        perm_importance = permutation_importance(\n",
    "            model, X_test, y_test, n_repeats=10, random_state=42, n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': feature_names,\n",
    "            'Importance': perm_importance.importances_mean,\n",
    "            'Std': perm_importance.importances_std\n",
    "        }).sort_values('Importance', ascending=False)\n",
    "        \n",
    "        ensemble_feature_importance[name] = {\n",
    "            'method': 'permutation',\n",
    "            'importance': feature_importance_df.to_dict('records')\n",
    "        }\n",
    "        \n",
    "        print(f\"   Method: Permutation Importance\")\n",
    "        print(f\"   Top 10 Features:\")\n",
    "        print(feature_importance_df[['Feature', 'Importance', 'Std']].head(10).to_string(index=False))\n",
    "\n",
    "# Save feature importance to JSON\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SAVING FEATURE IMPORTANCE TO JSON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, importance_data in ensemble_feature_importance.items():\n",
    "    json_file = f'../saved_ensemble_models/{name}_feature_importance.json'\n",
    "    \n",
    "    with open(json_file, 'w') as f:\n",
    "        json.dump(importance_data, f, indent=2)\n",
    "    \n",
    "    print(f\"   Saved: {name}_feature_importance.json\")\n",
    "\n",
    "print(\"\\nAll feature importance files saved to: models/saved_ensemble_models/\")\n",
    "\n",
    "# Cell 6.5: Visualize Ensemble Feature Importance\n",
    "num_models = len(ensemble_feature_importance)\n",
    "rows = (num_models + 1) // 2\n",
    "fig, axes = plt.subplots(rows, 2, figsize=(20, 6 * rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "colors_importance = ['#f59e0b', '#10b981', '#8b5cf6', '#ef4444', '#06b6d4', '#ec4899']\n",
    "\n",
    "for idx, (name, importance_data) in enumerate(ensemble_feature_importance.items()):\n",
    "    if idx >= len(axes):\n",
    "        break\n",
    "    \n",
    "    # Get top 10 features\n",
    "    top_10 = importance_data['importance'][:10]\n",
    "    features = [item['Feature'] for item in top_10]\n",
    "    importances = [item['Importance'] for item in top_10]\n",
    "    \n",
    "    axes[idx].barh(features, importances, color=colors_importance[idx % len(colors_importance)], \n",
    "                   alpha=0.8, edgecolor='black')\n",
    "    axes[idx].set_xlabel('Importance', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Features', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_title(f'{name.replace(\"_\", \" \").upper()}\\n({importance_data[\"method\"].capitalize()} Importance)',\n",
    "                       fontsize=12, fontweight='bold')\n",
    "    axes[idx].invert_yaxis()\n",
    "    axes[idx].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Hide unused subplots\n",
    "for idx in range(num_models, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cell 7: Visualize Ensemble Performance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 14))\n",
    "\n",
    "model_names = list(ensemble_results.keys())\n",
    "colors = ['#f59e0b', '#10b981', '#8b5cf6', '#ef4444', '#06b6d4', '#ec4899']\n",
    "\n",
    "# Plot 1: RMSE Comparison\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "cv_rmse = [ensemble_results[name]['cv_rmse'] for name in model_names]\n",
    "test_rmse = [ensemble_results[name]['test_rmse'] for name in model_names]\n",
    "\n",
    "axes[0, 0].bar(x - width/2, cv_rmse, width, label='CV RMSE', \n",
    "               color=colors[:len(model_names)], alpha=0.8)\n",
    "axes[0, 0].bar(x + width/2, test_rmse, width, label='Test RMSE', \n",
    "               color=colors[:len(model_names)], alpha=0.5)\n",
    "axes[0, 0].set_xlabel('Ensemble Models', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('RMSE', fontweight='bold')\n",
    "axes[0, 0].set_title('RMSE Comparison', fontweight='bold')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels([n.replace('_', ' ').upper() for n in model_names], \n",
    "                           rotation=25, ha='right', fontsize=7)\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: R² Score Comparison\n",
    "r2_scores = [ensemble_results[name]['test_r2'] for name in model_names]\n",
    "axes[0, 1].bar([n.replace('_', ' ').upper() for n in model_names], r2_scores, \n",
    "               color=colors[:len(model_names)], alpha=0.8)\n",
    "axes[0, 1].set_ylabel('R² Score', fontweight='bold')\n",
    "axes[0, 1].set_title('R² Score Comparison', fontweight='bold')\n",
    "axes[0, 1].set_xticklabels([n.replace('_', ' ').upper() for n in model_names], \n",
    "                           rotation=25, ha='right', fontsize=7)\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Plot 3 & 4: Prediction vs Actual for top 2 models\n",
    "sorted_models = sorted(model_names, key=lambda x: ensemble_results[x]['test_r2'], reverse=True)\n",
    "for idx, name in enumerate(sorted_models[:2]):\n",
    "    y_pred = ensemble_results[name]['y_pred']\n",
    "    \n",
    "    axes[1, idx].scatter(y_test, y_pred, alpha=0.6, color=colors[idx])\n",
    "    \n",
    "    min_val = min(y_test.min(), y_pred.min())\n",
    "    max_val = max(y_test.max(), y_pred.max())\n",
    "    axes[1, idx].plot([min_val, max_val], [min_val, max_val], \n",
    "                      'r--', linewidth=2, label='Perfect')\n",
    "    \n",
    "    axes[1, idx].set_xlabel('Actual', fontweight='bold')\n",
    "    axes[1, idx].set_ylabel('Predicted', fontweight='bold')\n",
    "    axes[1, idx].set_title(f'{name.replace(\"_\", \" \").upper()}\\nR²: {ensemble_results[name][\"test_r2\"]:.3f}',\n",
    "                          fontweight='bold')\n",
    "    axes[1, idx].legend()\n",
    "    axes[1, idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cell 8: Save Ensemble Models with Validation\n",
    "import pickle\n",
    "\n",
    "os.makedirs('../saved_ensemble_models', exist_ok=True)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SAVING ENSEMBLE MODELS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, data in ensemble_results.items():\n",
    "    model_file = f'../saved_ensemble_models/{name}_ensemble.pkl'\n",
    "    \n",
    "    try:\n",
    "        with open(model_file, 'wb') as f:\n",
    "            pickle.dump(data['model'], f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "        with open(model_file, 'rb') as f:\n",
    "            loaded_model = pickle.load(f)\n",
    "        \n",
    "        file_size = os.path.getsize(model_file)\n",
    "        print(f\"   [OK] {name}_ensemble.pkl saved and verified (Size: {file_size} bytes)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   [ERROR] Failed to save {name}_ensemble.pkl: {str(e)}\")\n",
    "\n",
    "print(\"\\nAll ensemble models saved to: models/saved_ensemble_models/\")\n",
    "\n",
    "# Cell 9: Compare Ensemble vs Base Models\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ENSEMBLE vs BASE MODELS COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "base_model_results = {}\n",
    "for model_name in ['knn', 'decision_tree', 'random_forest', 'svr', 'ridge', 'neural_network']:\n",
    "    try:\n",
    "        predictions = pd.read_csv(f'../saved_base_models/{model_name}_test_predictions.csv')\n",
    "        base_model_results[model_name] = {\n",
    "            'mae': predictions['Absolute_Error'].mean(),\n",
    "            'rmse': np.sqrt(np.mean(predictions['Error']**2))\n",
    "        }\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "all_results = []\n",
    "for name, metrics in base_model_results.items():\n",
    "    all_results.append({\n",
    "        'Type': 'Base',\n",
    "        'Model': name.upper(),\n",
    "        'RMSE': metrics['rmse'],\n",
    "        'MAE': metrics['mae']\n",
    "    })\n",
    "\n",
    "for name, data in ensemble_results.items():\n",
    "    all_results.append({\n",
    "        'Type': 'Ensemble',\n",
    "        'Model': name.replace('_', ' ').upper(),\n",
    "        'RMSE': data['test_rmse'],\n",
    "        'MAE': data['test_mae']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(all_results)\n",
    "comparison_df = comparison_df.sort_values('RMSE')\n",
    "\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 7))\n",
    "\n",
    "x = np.arange(len(comparison_df))\n",
    "colors_bar = ['skyblue' if t == 'Base' else 'orange' for t in comparison_df['Type']]\n",
    "bars = ax.bar(x, comparison_df['RMSE'], color=colors_bar, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Models', fontweight='bold')\n",
    "ax.set_ylabel('RMSE', fontweight='bold')\n",
    "ax.set_title('All Models RMSE Comparison (Base vs Ensemble)', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df['Model'], rotation=45, ha='right')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='skyblue', alpha=0.7, label='Base Models'),\n",
    "                   Patch(facecolor='orange', alpha=0.7, label='Ensemble Models')]\n",
    "ax.legend(handles=legend_elements)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_overall = comparison_df.iloc[0]\n",
    "print(f\"\\nBest Overall Model: {best_overall['Model']}\")\n",
    "print(f\"   Type: {best_overall['Type']}\")\n",
    "print(f\"   RMSE: {best_overall['RMSE']:.4f}\")\n",
    "print(f\"   MAE: {best_overall['MAE']:.4f}\")\n",
    "\n",
    "# Cell 10: Compare Stacking Final Estimators\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"STACKING FINAL ESTIMATOR COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "stacking_models = {k: v for k, v in ensemble_results.items() if 'stacking' in k}\n",
    "\n",
    "stacking_comparison = []\n",
    "for name, data in stacking_models.items():\n",
    "    final_estimator_type = 'Neural Network' if 'neural_final' in name or 'neural_only' in name else 'Ridge'\n",
    "    stacking_comparison.append({\n",
    "        'Model': name.replace('_', ' ').upper(),\n",
    "        'Final Estimator': final_estimator_type,\n",
    "        'Test RMSE': data['test_rmse'],\n",
    "        'CV RMSE': data['cv_rmse'],\n",
    "        'Test R²': data['test_r2']\n",
    "    })\n",
    "\n",
    "stacking_df = pd.DataFrame(stacking_comparison)\n",
    "stacking_df = stacking_df.sort_values('Test RMSE')\n",
    "\n",
    "print(\"\\n\" + stacking_df.to_string(index=False))\n",
    "\n",
    "print(\"\\nAnalysis:\")\n",
    "print(\"Ridge as final estimator generally provides more stable predictions\")\n",
    "print(\"Neural Network as final estimator can capture complex patterns but may overfit\")\n",
    "print(\"Choose Ridge for better generalization, Neural Network for complex data patterns\")\n",
    "\n",
    "# Cell 11: Save Ensemble Model Metadata for Database Import\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SAVING ENSEMBLE MODEL METADATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, data in ensemble_results.items():\n",
    "    metadata = {\n",
    "        'model_name': name,\n",
    "        'model_type': 'ensemble_regression',\n",
    "        'cv_rmse': float(data['cv_rmse']),\n",
    "        'cv_std': float(data['cv_mse_std']),\n",
    "        'cv_mean': float(data['cv_mse_mean']),\n",
    "        'test_rmse': float(data['test_rmse']),\n",
    "        'test_mae': float(data['test_mae']),\n",
    "        'test_r2': float(data['test_r2']),\n",
    "        'test_mse': float(data['test_mse']),\n",
    "        'cv_10fold_scores': data['cv_10fold_scores'],\n",
    "        'trained_at': pd.Timestamp.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    metadata_file = f'../regression_processed_data/{name}_metadata.json'\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"   Saved metadata: {metadata_file}\")\n",
    "\n",
    "print(\"\\nMetadata files saved to: models/regression_processed_data/\")\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ENSEMBLE MODELS TRAINING COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\" Feature importance calculated and saved to JSON for all models\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
