{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a720ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully\")\n",
    "\n",
    "# Cell 2: Preprocessed Data\n",
    "X_train = np.load('../preprocessed_dataset/X_train.npy')\n",
    "X_test = np.load('../preprocessed_dataset/X_test.npy')\n",
    "y_train = np.load('../preprocessed_dataset/y_train.npy')\n",
    "y_test = np.load('../preprocessed_dataset/y_test.npy')\n",
    "\n",
    "print(f\" Data Loaded:\")\n",
    "print(f\"   Training samples: {X_train.shape[0]}\")\n",
    "print(f\"   Test samples: {X_test.shape[0]}\")\n",
    "print(f\"   Features: {X_train.shape[1]}\")\n",
    "print(f\"   Target range: [{y_train.min():.2f}, {y_train.max():.2f}]\")\n",
    "\n",
    "# Cell 3: Define Model Configurations\n",
    "model_configs = {\n",
    "    'knn': {\n",
    "        'model': KNeighborsRegressor(),\n",
    "        'params': {\n",
    "            'n_neighbors': [3, 5, 7, 9],\n",
    "            'weights': ['uniform', 'distance']\n",
    "        }\n",
    "    },\n",
    "    'decision_tree': {\n",
    "        'model': DecisionTreeRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'max_depth': [3, 5, 7, 10, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'model': RandomForestRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100, 200],\n",
    "            'max_depth': [5, 10, 15, None],\n",
    "            'min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "        'svr': {\n",
    "        'model': SVR(),\n",
    "        'params': {\n",
    "            'C': [0.1, 1.0, 10.0],\n",
    "            'epsilon': [0.01, 0.1, 0.2],\n",
    "            'kernel': ['rbf']\n",
    "        }\n",
    "    },\n",
    "    'ridge': {\n",
    "        'model': Ridge(),\n",
    "        'params': {\n",
    "            'alpha': [0.1, 1.0, 10.0, 100.0]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"ðŸ”§ Model Configurations:\")\n",
    "for name in model_configs.keys():\n",
    "    print(f\"   âœ“ {name.upper()}\")\n",
    "\n",
    "# Cell 4: Train Models with 10-Fold CV\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "results = {}\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAINING BASE MODELS WITH 10-FOLD CROSS-VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, config in model_configs.items():\n",
    "    print(f\"\\n Training {name.upper()}...\")\n",
    "    \n",
    "    grid_search = GridSearchCV(\n",
    "        config['model'],\n",
    "        config['params'],\n",
    "        cv=cv,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_train, y_train)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    cv_results = grid_search.cv_results_\n",
    "    best_index = grid_search.best_index_\n",
    "    \n",
    "    fold_scores = []\n",
    "    for fold_idx in range(10):\n",
    "        fold_key = f'split{fold_idx}_test_score'\n",
    "        fold_scores.append(-cv_results[fold_key][best_index])\n",
    "    \n",
    "    cv_mean = np.mean(fold_scores)\n",
    "    cv_std = np.std(fold_scores)\n",
    "    cv_min = np.min(fold_scores)\n",
    "    cv_max = np.max(fold_scores)\n",
    "    \n",
    "    cv_rmse = np.sqrt(cv_mean)\n",
    "    \n",
    "    y_pred = best_model.predict(X_test)\n",
    "    test_mse = mean_squared_error(y_test, y_pred)\n",
    "    test_rmse = np.sqrt(test_mse)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred)\n",
    "    test_r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    results[name] = {\n",
    "        'model': best_model,\n",
    "        'cv_mse_mean': cv_mean,\n",
    "        'cv_mse_std': cv_std,\n",
    "        'cv_mse_min': cv_min,\n",
    "        'cv_mse_max': cv_max,\n",
    "        'cv_rmse': cv_rmse,\n",
    "        'cv_10fold_scores': fold_scores,\n",
    "        'test_mse': test_mse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mae': test_mae,\n",
    "        'test_r2': test_r2,\n",
    "        'y_pred': y_pred,\n",
    "        'best_params': grid_search.best_params_\n",
    "    }\n",
    "    \n",
    "    print(f\"   âœ… {name.upper()} Trained\")\n",
    "    print(f\"      10-Fold CV RMSE: {cv_rmse:.4f} Â± {np.sqrt(cv_std):.4f}\")\n",
    "    print(f\"      Test RMSE: {test_rmse:.4f}\")\n",
    "    print(f\"      Test MAE: {test_mae:.4f}\")\n",
    "    print(f\"      Test RÂ²: {test_r2:.4f}\")\n",
    "    print(f\"      Best Params: {grid_search.best_params_}\")\n",
    "\n",
    "# Cell 5: Visualize 10-Fold CV Results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "model_names = list(results.keys())\n",
    "colors = ['#2ecc71', '#3498db', '#e74c3c',  '#9b59b6', '#f39c12']\n",
    "\n",
    "# Plot 1: Box plot of 10-fold scores (RMSE)\n",
    "fold_data = []\n",
    "for name in model_names:\n",
    "    for fold_idx, mse in enumerate(results[name]['cv_10fold_scores'], 1):\n",
    "        fold_data.append({\n",
    "            'Model': name.upper(),\n",
    "            'Fold': fold_idx,\n",
    "            'RMSE': np.sqrt(mse)\n",
    "        })\n",
    "\n",
    "fold_df = pd.DataFrame(fold_data)\n",
    "\n",
    "box_positions = []\n",
    "for idx, name in enumerate(model_names):\n",
    "    model_data = fold_df[fold_df['Model'] == name.upper()]['RMSE']\n",
    "    bp = axes[0, 0].boxplot([model_data], positions=[idx], widths=0.6,\n",
    "                            patch_artist=True,\n",
    "                            boxprops=dict(facecolor=colors[idx], alpha=0.7),\n",
    "                            medianprops=dict(color='black', linewidth=2))\n",
    "    box_positions.append(idx)\n",
    "\n",
    "axes[0, 0].set_xticks(box_positions)\n",
    "axes[0, 0].set_xticklabels([name.upper() for name in model_names])\n",
    "axes[0, 0].set_ylabel('RMSE', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_title('10-Fold Cross-Validation RMSE Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 2: Line plot showing each fold's performance\n",
    "for idx, name in enumerate(model_names):\n",
    "    fold_rmse = [np.sqrt(mse) for mse in results[name]['cv_10fold_scores']]\n",
    "    axes[0, 1].plot(range(1, 11), fold_rmse, marker='o', linewidth=2,\n",
    "                   label=name.upper(), color=colors[idx])\n",
    "\n",
    "axes[0, 1].set_xlabel('Fold Number', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_ylabel('RMSE', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_title('RMSE Across 10 Folds', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "axes[0, 1].set_xticks(range(1, 11))\n",
    "\n",
    "# Plot 3: CV RMSE vs Test RMSE comparison\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "cv_rmse_values = [results[name]['cv_rmse'] for name in model_names]\n",
    "test_rmse_values = [results[name]['test_rmse'] for name in model_names]\n",
    "\n",
    "bars1 = axes[1, 0].bar(x - width/2, cv_rmse_values, width, label='10-Fold CV RMSE',\n",
    "                       color=colors, alpha=0.8)\n",
    "bars2 = axes[1, 0].bar(x + width/2, test_rmse_values, width, label='Test RMSE',\n",
    "                       color=colors, alpha=0.5)\n",
    "\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{height:.2f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "axes[1, 0].set_xlabel('Models', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('RMSE', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_title('10-Fold CV RMSE vs Test RMSE', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels([name.upper() for name in model_names])\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 4: RÂ² Score comparison\n",
    "r2_scores = [results[name]['test_r2'] for name in model_names]\n",
    "\n",
    "bars = axes[1, 1].bar([name.upper() for name in model_names], r2_scores,\n",
    "                      color=colors, alpha=0.8)\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                   f'{height:.4f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "axes[1, 1].set_xlabel('Models', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('RÂ² Score', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_title('RÂ² Score (Higher is Better)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "axes[1, 1].set_ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cell 6: Performance Summary Table\n",
    "summary_data = []\n",
    "for name in model_names:\n",
    "    summary_data.append({\n",
    "        'Model': name.upper(),\n",
    "        'CV RMSE': f\"{results[name]['cv_rmse']:.4f}\",\n",
    "        'Test RMSE': f\"{results[name]['test_rmse']:.4f}\",\n",
    "        'Test MAE': f\"{results[name]['test_mae']:.4f}\",\n",
    "        'Test RÂ²': f\"{results[name]['test_r2']:.4f}\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "display(summary_df)\n",
    "\n",
    "best_model_name = min(results.items(), key=lambda x: x[1]['test_rmse'])[0]\n",
    "print(f\"\\nBest Model: {best_model_name.upper()}\")\n",
    "print(f\"   Test RMSE: {results[best_model_name]['test_rmse']:.4f}\")\n",
    "print(f\"   Test RÂ²: {results[best_model_name]['test_r2']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DETAILED TEST RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name in model_names:\n",
    "    y_pred = results[name]['y_pred']\n",
    "    rmse = results[name]['test_rmse']\n",
    "    mae = results[name]['test_mae']\n",
    "    r2 = results[name]['test_r2']\n",
    "    \n",
    "    print(f\"\\n{name.upper()}\")\n",
    "    print(f\"   Test RMSE: {rmse:.4f}\")\n",
    "    print(f\"   Test MAE:  {mae:.4f}\")\n",
    "    print(f\"   Test RÂ²:   {r2:.4f}\")\n",
    "    print(f\"   10-Fold CV RMSE: {results[name]['cv_rmse']:.4f} Â± {results[name]['cv_mse_std']:.4f}\")\n",
    "    \n",
    "    residuals = y_test - y_pred\n",
    "    print(f\"   Mean Residual: {np.mean(residuals):.4f}\")\n",
    "    print(f\"   Std Residual:  {np.std(residuals):.4f}\")\n",
    "\n",
    "# Cell 6.5: Detailed Test Predictions Analysis\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TEST PREDICTIONS ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name in model_names:\n",
    "    y_pred = results[name]['y_pred']\n",
    "    \n",
    "    prediction_details = pd.DataFrame({\n",
    "        'Actual': y_test,\n",
    "        'Predicted': y_pred,\n",
    "        'Error': y_test - y_pred,\n",
    "        'Absolute_Error': np.abs(y_test - y_pred),\n",
    "        'Percent_Error': np.abs((y_test - y_pred) / y_test) * 100\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{name.upper()}\")\n",
    "    print(f\"   Total Test Samples: {len(y_test)}\")\n",
    "    print(f\"   Mean Absolute Error: {prediction_details['Absolute_Error'].mean():.4f}\")\n",
    "    print(f\"   Median Absolute Error: {prediction_details['Absolute_Error'].median():.4f}\")\n",
    "    print(f\"   Max Error: {prediction_details['Absolute_Error'].max():.4f}\")\n",
    "    print(f\"   Min Error: {prediction_details['Absolute_Error'].min():.4f}\")\n",
    "    \n",
    "    print(f\"\\n   First 10 predictions:\")\n",
    "    print(prediction_details.head(10).to_string(index=False))\n",
    "    \n",
    "    csv_file = f'../saved_base_models_dataset/{name}_test_predictions.csv'\n",
    "    prediction_details.to_csv(csv_file, index=False)\n",
    "    print(f\"\\n   Saved detailed predictions: {csv_file}\")\n",
    "\n",
    "# Cell 7: Prediction vs Actual Plots\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, name in enumerate(model_names):\n",
    "    y_pred = results[name]['y_pred']\n",
    "    \n",
    "    # Scatter plot\n",
    "    axes[idx].scatter(y_test, y_pred, alpha=0.6, s=50, color=colors[idx])\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(y_test.min(), y_pred.min())\n",
    "    max_val = max(y_test.max(), y_pred.max())\n",
    "    axes[idx].plot([min_val, max_val], [min_val, max_val], \n",
    "                   'r--', linewidth=2, label='Perfect Prediction')\n",
    "    \n",
    "    # Metrics\n",
    "    rmse = results[name]['test_rmse']\n",
    "    r2 = results[name]['test_r2']\n",
    "    \n",
    "    axes[idx].set_xlabel('Actual Exam Score', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_ylabel('Predicted Exam Score', fontsize=11, fontweight='bold')\n",
    "    axes[idx].set_title(f'{name.upper()}\\nRMSE: {rmse:.2f}, RÂ²: {r2:.3f}',\n",
    "                       fontsize=12, fontweight='bold')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(alpha=0.3)\n",
    "\n",
    "axes[5].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cell 8: Feature Importance\n",
    "if 'random_forest' in results:\n",
    "    feature_names = json.load(open('../preprocessed_dataset/feature_names.json'))\n",
    "    \n",
    "    rf_model = results['random_forest']['model']\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': rf_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(feature_importance['Feature'][:10], \n",
    "             feature_importance['Importance'][:10],\n",
    "             color='skyblue', edgecolor='black')\n",
    "    plt.xlabel('Importance', fontsize=12, fontweight='bold')\n",
    "    plt.ylabel('Features', fontsize=12, fontweight='bold')\n",
    "    plt.title('Top 10 Feature Importance (Random Forest)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.grid(axis='x', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n Feature Importance:\")\n",
    "    display(feature_importance)\n",
    "\n",
    "# Cell 9: Save Models\n",
    "os.makedirs('../saved_base_models_dataset', exist_ok=True)\n",
    "\n",
    "for name, data in results.items():\n",
    "    joblib.dump(data['model'], f'../saved_base_models_dataset/{name}_model.pkl')\n",
    "\n",
    "print(\"\\n Models saved to saved_base_models_dataset/\")\n",
    "print(\" BASE MODELS TRAINING COMPLETE!\")\n",
    "\n",
    "# Cell 10: Create and Save Preprocessor\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "models_path = os.path.abspath('..')\n",
    "if models_path not in sys.path:\n",
    "    sys.path.insert(0, models_path)\n",
    "\n",
    "# Import from the correct module path\n",
    "from regression_preprocessor import RegressionPreprocessor\n",
    "\n",
    "label_encoders = joblib.load('../preprocessed_dataset/label_encoders.pkl')\n",
    "imputer = joblib.load('../preprocessed_dataset/imputer.pkl')\n",
    "scaler = joblib.load('../preprocessed_dataset/scaler.pkl')\n",
    "feature_names = json.load(open('../preprocessed_dataset/feature_names.json'))\n",
    "\n",
    "preprocessor = RegressionPreprocessor(imputer, label_encoders, scaler)\n",
    "\n",
    "joblib.dump(preprocessor, '../saved_base_models_dataset/preprocessor.pkl')\n",
    "joblib.dump(feature_names, '../saved_base_models_dataset/feature_names.pkl')\n",
    "\n",
    "print(\"\\nPreprocessor saved to ../saved_base_models_dataset/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
