{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c16160d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import os\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "print(\"âœ… All libraries imported successfully\")\n",
    "\n",
    "# Cell 2: Load Data and Base Models\n",
    "X_train = np.load('../preprocessed_dataset/X_train.npy')\n",
    "X_test = np.load('../preprocessed_dataset/X_test.npy')\n",
    "y_train = np.load('../preprocessed_dataset/y_train.npy')\n",
    "y_test = np.load('../preprocessed_dataset/y_test.npy')\n",
    "\n",
    "# Load base models\n",
    "base_models = {}\n",
    "for model_name in ['knn', 'decision_tree', 'random_forest', 'svr', 'ridge']:\n",
    "    base_models[model_name] = joblib.load(f'../saved_base_models_dataset/{model_name}_model.pkl')\n",
    "\n",
    "print(f\"ðŸ“Š Data Loaded:\")\n",
    "print(f\"   Training: {X_train.shape}, Test: {X_test.shape}\")\n",
    "print(f\"âœ… Base models loaded: {list(base_models.keys())}\")\n",
    "\n",
    "# Cell 3: Create Ensemble Models\n",
    "ensemble_models = {\n",
    "    'bagging_random_forest': RandomForestRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        min_samples_split=5,\n",
    "        random_state=42,\n",
    "        bootstrap=True\n",
    "    ),\n",
    "    'boosting_gradient_boost': GradientBoostingRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=3,\n",
    "        random_state=42\n",
    "    ),\n",
    "    'stacking_ridge': StackingRegressor(\n",
    "        estimators=[\n",
    "            ('random_forest', base_models['random_forest']),\n",
    "            ('ridge', base_models['ridge'])\n",
    "        ],\n",
    "        final_estimator=Ridge(alpha=1.0),\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "}\n",
    "\n",
    "print(\"Ensemble Models Created:\")\n",
    "for name in ensemble_models.keys():\n",
    "    print(f\"   {name.replace('_', ' ').upper()}\")\n",
    "\n",
    "# Cell 4: Train Ensemble Models with 10-Fold CV\n",
    "X_full = np.vstack([X_train, X_test])\n",
    "y_full = np.concatenate([y_train, y_test])\n",
    "cv = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "ensemble_results = {}\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TRAINING ENSEMBLE MODELS WITH 10-FOLD CROSS-VALIDATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, model in ensemble_models.items():\n",
    "    print(f\"\\n[MODEL] Training {name.upper()} with 10-fold CV...\")\n",
    "    \n",
    "    fold_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    \n",
    "    cv_mse_scores = -fold_scores\n",
    "    cv_mean = np.mean(cv_mse_scores)\n",
    "    cv_std = np.std(cv_mse_scores)\n",
    "    cv_rmse = np.sqrt(cv_mean)\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Test predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    test_mse = mean_squared_error(y_test, y_pred)\n",
    "    test_rmse = np.sqrt(test_mse)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred)\n",
    "    test_r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    ensemble_results[name] = {\n",
    "        'model': model,\n",
    "        'cv_mse_mean': cv_mean,\n",
    "        'cv_mse_std': cv_std,\n",
    "        'cv_rmse': cv_rmse,\n",
    "        'cv_10fold_scores': fold_scores.tolist(),\n",
    "        'test_mse': test_mse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'test_mae': test_mae,\n",
    "        'test_r2': test_r2,\n",
    "        'y_pred': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"   âœ… {name.replace('_', ' ').upper()} Trained\")\n",
    "    print(f\"      10-Fold CV RMSE: {cv_rmse:.4f}\")\n",
    "    print(f\"      Test RMSE: {test_rmse:.4f}\")\n",
    "    print(f\"      Test RÂ²: {test_r2:.4f}\")\n",
    "\n",
    "# Cell 5: Compare Ensemble Models\n",
    "summary_data = []\n",
    "for name, data in ensemble_results.items():\n",
    "    summary_data.append({\n",
    "        'Model': name.replace('_', ' ').upper(),\n",
    "        'CV RMSE': f\"{data['cv_rmse']:.4f}\",\n",
    "        'Test RMSE': f\"{data['test_rmse']:.4f}\",\n",
    "        'Test MAE': f\"{data['test_mae']:.4f}\",\n",
    "        'Test RÂ²': f\"{data['test_r2']:.4f}\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ENSEMBLE MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "display(summary_df)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DETAILED ENSEMBLE TEST RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "sorted_models = sorted(ensemble_results.items(), key=lambda x: x[1]['test_rmse'])\n",
    "\n",
    "print(f\"\\n{'Rank':<5} {'Model':<30} {'Test RMSE':<12} {'CV RMSE':<12} {'RÂ²':<10}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for rank, (name, data) in enumerate(sorted_models, 1):\n",
    "    print(f\"{rank:<5} {name.replace('_', ' ').upper():<30} \"\n",
    "          f\"{data['test_rmse']:<12.4f} {data['cv_rmse']:<12.4f} {data['test_r2']:<10.4f}\")\n",
    "\n",
    "best_model = sorted_models[0]\n",
    "print(f\"\\nBest Ensemble Model: {best_model[0].replace('_', ' ').upper()}\")\n",
    "print(f\"   Test RMSE: {best_model[1]['test_rmse']:.4f}\")\n",
    "print(f\"   Test RÂ²: {best_model[1]['test_r2']:.4f}\")\n",
    "print(f\"   10-Fold CV RMSE: {best_model[1]['cv_rmse']:.4f}\")\n",
    "\n",
    "# Cell 5.5: Detailed Ensemble Predictions Analysis\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ENSEMBLE TEST PREDICTIONS ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for name, data in ensemble_results.items():\n",
    "    y_pred = data['y_pred']\n",
    "    \n",
    "    prediction_details = pd.DataFrame({\n",
    "        'Actual': y_test,\n",
    "        'Predicted': y_pred,\n",
    "        'Error': y_test - y_pred,\n",
    "        'Absolute_Error': np.abs(y_test - y_pred),\n",
    "        'Percent_Error': np.abs((y_test - y_pred) / y_test) * 100\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n{name.replace('_', ' ').upper()}\")\n",
    "    print(f\"   Total Test Samples: {len(y_test)}\")\n",
    "    print(f\"   Test RMSE: {data['test_rmse']:.4f}\")\n",
    "    print(f\"   Test MAE:  {data['test_mae']:.4f}\")\n",
    "    print(f\"   Test RÂ²:   {data['test_r2']:.4f}\")\n",
    "    print(f\"   10-Fold CV RMSE: {data['cv_rmse']:.4f}\")\n",
    "    \n",
    "    residuals = y_test - y_pred\n",
    "    print(f\"   Mean Residual: {np.mean(residuals):.4f}\")\n",
    "    print(f\"   Std Residual:  {np.std(residuals):.4f}\")\n",
    "    \n",
    "    print(f\"\\n   Error Statistics:\")\n",
    "    print(f\"   Mean Absolute Error: {prediction_details['Absolute_Error'].mean():.4f}\")\n",
    "    print(f\"   Median Absolute Error: {prediction_details['Absolute_Error'].median():.4f}\")\n",
    "    print(f\"   Max Error: {prediction_details['Absolute_Error'].max():.4f}\")\n",
    "    print(f\"   Min Error: {prediction_details['Absolute_Error'].min():.4f}\")\n",
    "    \n",
    "    print(f\"\\n   First 10 predictions:\")\n",
    "    print(prediction_details.head(10).to_string(index=False))\n",
    "    \n",
    "    large_errors = prediction_details[prediction_details['Absolute_Error'] > data['test_rmse']]\n",
    "    if len(large_errors) > 0:\n",
    "        print(f\"\\n   Samples with error > RMSE ({len(large_errors)} samples):\")\n",
    "        print(large_errors.head(5).to_string(index=False))\n",
    "    \n",
    "    csv_file = f'../saved_ensemble_models_dataset/{name}_test_predictions.csv'\n",
    "    prediction_details.to_csv(csv_file, index=False)\n",
    "    print(f\"\\n   Saved detailed predictions: {csv_file}\")\n",
    "\n",
    "# Cell 6: Visualize Ensemble Performance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "model_names = list(ensemble_results.keys())\n",
    "colors = ['#f59e0b', '#10b981', '#8b5cf6']\n",
    "\n",
    "# Plot 1: RMSE Comparison\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "cv_rmse = [ensemble_results[name]['cv_rmse'] for name in model_names]\n",
    "test_rmse = [ensemble_results[name]['test_rmse'] for name in model_names]\n",
    "\n",
    "axes[0, 0].bar(x - width/2, cv_rmse, width, label='CV RMSE', color=colors, alpha=0.8)\n",
    "axes[0, 0].bar(x + width/2, test_rmse, width, label='Test RMSE', color=colors, alpha=0.5)\n",
    "axes[0, 0].set_xlabel('Ensemble Models', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('RMSE', fontweight='bold')\n",
    "axes[0, 0].set_title('RMSE Comparison', fontweight='bold')\n",
    "axes[0, 0].set_xticks(x)\n",
    "axes[0, 0].set_xticklabels([n.replace('_', ' ').upper() for n in model_names], rotation=15, ha='right')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# Plot 2: RÂ² Score Comparison\n",
    "r2_scores = [ensemble_results[name]['test_r2'] for name in model_names]\n",
    "axes[0, 1].bar([n.replace('_', ' ').upper() for n in model_names], r2_scores, \n",
    "               color=colors, alpha=0.8)\n",
    "axes[0, 1].set_ylabel('RÂ² Score', fontweight='bold')\n",
    "axes[0, 1].set_title('RÂ² Score Comparison', fontweight='bold')\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# Plot 3 & 4: Prediction vs Actual \n",
    "sorted_models = sorted(model_names, key=lambda x: ensemble_results[x]['test_r2'], reverse=True)\n",
    "for idx, name in enumerate(sorted_models[:2]):\n",
    "    y_pred = ensemble_results[name]['y_pred']\n",
    "    \n",
    "    axes[1, idx].scatter(y_test, y_pred, alpha=0.6, color=colors[idx])\n",
    "    \n",
    "    min_val = min(y_test.min(), y_pred.min())\n",
    "    max_val = max(y_test.max(), y_pred.max())\n",
    "    axes[1, idx].plot([min_val, max_val], [min_val, max_val], \n",
    "                      'r--', linewidth=2, label='Perfect')\n",
    "    \n",
    "    axes[1, idx].set_xlabel('Actual', fontweight='bold')\n",
    "    axes[1, idx].set_ylabel('Predicted', fontweight='bold')\n",
    "    axes[1, idx].set_title(f'{name.replace(\"_\", \" \").upper()}\\nRÂ²: {ensemble_results[name][\"test_r2\"]:.3f}',\n",
    "                          fontweight='bold')\n",
    "    axes[1, idx].legend()\n",
    "    axes[1, idx].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cell 7: Save Ensemble Models\n",
    "os.makedirs('../saved_ensemble_models_dataset', exist_ok=True)\n",
    "\n",
    "for name, data in ensemble_results.items():\n",
    "    joblib.dump(data['model'], f'../saved_ensemble_models_dataset/{name}_ensemble.pkl')\n",
    "\n",
    "print(\"\\nðŸ’¾ Ensemble models saved to ../saved_ensemble_models_dataset/\")\n",
    "\n",
    "# Cell 7.5: Compare Ensemble vs Base Models\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ENSEMBLE vs BASE MODELS COMPARISON\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "base_model_results = {}\n",
    "for model_name in ['knn', 'decision_tree', 'random_forest', 'svr', 'ridge']:\n",
    "    try:\n",
    "        predictions = pd.read_csv(f'../saved_base_models_dataset/{model_name}_test_predictions.csv')\n",
    "        base_model_results[model_name] = {\n",
    "            'mae': predictions['Absolute_Error'].mean(),\n",
    "            'rmse': np.sqrt(np.mean(predictions['Error']**2))\n",
    "        }\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "all_results = []\n",
    "for name, metrics in base_model_results.items():\n",
    "    all_results.append({\n",
    "        'Type': 'Base',\n",
    "        'Model': name.upper(),\n",
    "        'RMSE': metrics['rmse'],\n",
    "        'MAE': metrics['mae']\n",
    "    })\n",
    "\n",
    "for name, data in ensemble_results.items():\n",
    "    all_results.append({\n",
    "        'Type': 'Ensemble',\n",
    "        'Model': name.replace('_', ' ').upper(),\n",
    "        'RMSE': data['test_rmse'],\n",
    "        'MAE': data['test_mae']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(all_results)\n",
    "comparison_df = comparison_df.sort_values('RMSE')\n",
    "\n",
    "print(\"\\n\" + comparison_df.to_string(index=False))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "base_models_df = comparison_df[comparison_df['Type'] == 'Base']\n",
    "ensemble_df = comparison_df[comparison_df['Type'] == 'Ensemble']\n",
    "\n",
    "x = np.arange(len(comparison_df))\n",
    "width = 0.35\n",
    "\n",
    "colors = ['skyblue' if t == 'Base' else 'orange' for t in comparison_df['Type']]\n",
    "bars = ax.bar(x, comparison_df['RMSE'], color=colors, alpha=0.7)\n",
    "\n",
    "ax.set_xlabel('Models', fontweight='bold')\n",
    "ax.set_ylabel('RMSE', fontweight='bold')\n",
    "ax.set_title('All Models RMSE Comparison (Base vs Ensemble)', fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison_df['Model'], rotation=45, ha='right')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor='skyblue', alpha=0.7, label='Base Models'),\n",
    "                   Patch(facecolor='orange', alpha=0.7, label='Ensemble Models')]\n",
    "ax.legend(handles=legend_elements)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "best_overall = comparison_df.iloc[0]\n",
    "print(f\"\\nBest Overall Model: {best_overall['Model']}\")\n",
    "print(f\"   Type: {best_overall['Type']}\")\n",
    "print(f\"   RMSE: {best_overall['RMSE']:.4f}\")\n",
    "print(f\"   MAE: {best_overall['MAE']:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
